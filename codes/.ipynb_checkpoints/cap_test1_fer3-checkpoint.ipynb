{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# code credits: Durgesh Thakur\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../test/train'\n",
    "validation_data_dir = '../test/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "img_rows,img_cols = 48,48\n",
    "batch_size = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=30,\n",
    "                                   shear_range=0.3,\n",
    "                                   zoom_range=0.3,\n",
    "                                   width_shift_range=0.4,\n",
    "                                   height_shift_range=0.4,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16944 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    color_mode='grayscale',\n",
    "                                                    target_size=(img_rows,img_cols),\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2099 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                              color_mode='grayscale',\n",
    "                                                              target_size=(img_rows,img_cols),\n",
    "                                                              batch_size=batch_size,\n",
    "                                                              class_mode='categorical',\n",
    "                                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,327,907\n",
      "Trainable params: 1,325,731\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# layers\n",
    "model = Sequential()\n",
    "\n",
    "# Block-1\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-2 \n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-3\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-4 \n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-5\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block-6\n",
    "\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block-7\n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks\n",
    "checkpoint = ModelCheckpoint('rong_test1_fer3.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "# earlystop = EarlyStopping(monitor='val_loss',\n",
    "#                           min_delta=0,\n",
    "#                           patience=9,\n",
    "#                           verbose=1,\n",
    "#                           restore_best_weights=True\n",
    "#                           )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "# callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "callbacks = [checkpoint,reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-c7c7309de9d4>:11: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.6064 - accuracy: 0.3555\n",
      "Epoch 00001: val_loss improved from inf to 1.20494, saving model to rong_test1_fer3.h5\n",
      "141/141 [==============================] - 26s 183ms/step - loss: 1.6064 - accuracy: 0.3555 - val_loss: 1.2049 - val_accuracy: 0.4123 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.3151 - accuracy: 0.3677\n",
      "Epoch 00002: val_loss improved from 1.20494 to 1.09993, saving model to rong_test1_fer3.h5\n",
      "141/141 [==============================] - 43s 303ms/step - loss: 1.3151 - accuracy: 0.3677 - val_loss: 1.0999 - val_accuracy: 0.4186 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.2125 - accuracy: 0.3763\n",
      "Epoch 00003: val_loss did not improve from 1.09993\n",
      "141/141 [==============================] - 44s 309ms/step - loss: 1.2125 - accuracy: 0.3763 - val_loss: 1.1113 - val_accuracy: 0.4191 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.1394 - accuracy: 0.3961\n",
      "Epoch 00004: val_loss improved from 1.09993 to 1.09633, saving model to rong_test1_fer3.h5\n",
      "141/141 [==============================] - 45s 316ms/step - loss: 1.1394 - accuracy: 0.3961 - val_loss: 1.0963 - val_accuracy: 0.4206 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.1201 - accuracy: 0.3947\n",
      "Epoch 00005: val_loss improved from 1.09633 to 1.08461, saving model to rong_test1_fer3.h5\n",
      "141/141 [==============================] - 46s 324ms/step - loss: 1.1201 - accuracy: 0.3947 - val_loss: 1.0846 - val_accuracy: 0.4152 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.1064 - accuracy: 0.3988\n",
      "Epoch 00006: val_loss improved from 1.08461 to 1.08318, saving model to rong_test1_fer3.h5\n",
      "141/141 [==============================] - 45s 322ms/step - loss: 1.1064 - accuracy: 0.3988 - val_loss: 1.0832 - val_accuracy: 0.4196 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0929 - accuracy: 0.4096\n",
      "Epoch 00007: val_loss improved from 1.08318 to 1.07963, saving model to rong_test1_fer3.h5\n",
      "141/141 [==============================] - 48s 337ms/step - loss: 1.0929 - accuracy: 0.4096 - val_loss: 1.0796 - val_accuracy: 0.4196 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0845 - accuracy: 0.4191\n",
      "Epoch 00008: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 47s 332ms/step - loss: 1.0845 - accuracy: 0.4191 - val_loss: 1.0798 - val_accuracy: 0.4186 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0803 - accuracy: 0.4391\n",
      "Epoch 00009: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 46s 329ms/step - loss: 1.0803 - accuracy: 0.4391 - val_loss: 1.0819 - val_accuracy: 0.4181 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0773 - accuracy: 0.4337\n",
      "Epoch 00010: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "141/141 [==============================] - 47s 334ms/step - loss: 1.0773 - accuracy: 0.4337 - val_loss: 1.0807 - val_accuracy: 0.4191 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0796 - accuracy: 0.4255\n",
      "Epoch 00011: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 56s 396ms/step - loss: 1.0796 - accuracy: 0.4255 - val_loss: 1.0826 - val_accuracy: 0.4176 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0738 - accuracy: 0.4357\n",
      "Epoch 00012: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 52s 370ms/step - loss: 1.0738 - accuracy: 0.4357 - val_loss: 1.0814 - val_accuracy: 0.4221 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0745 - accuracy: 0.4313\n",
      "Epoch 00013: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0745 - accuracy: 0.4313 - val_loss: 1.0817 - val_accuracy: 0.4225 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0783 - accuracy: 0.4226\n",
      "Epoch 00014: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 51s 361ms/step - loss: 1.0783 - accuracy: 0.4226 - val_loss: 1.0816 - val_accuracy: 0.4230 - lr: 4.0000e-05\n",
      "Epoch 15/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0752 - accuracy: 0.4353\n",
      "Epoch 00015: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 48s 337ms/step - loss: 1.0752 - accuracy: 0.4353 - val_loss: 1.0822 - val_accuracy: 0.4221 - lr: 4.0000e-05\n",
      "Epoch 16/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0743 - accuracy: 0.4273\n",
      "Epoch 00016: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0743 - accuracy: 0.4273 - val_loss: 1.0820 - val_accuracy: 0.4206 - lr: 4.0000e-05\n",
      "Epoch 17/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0717 - accuracy: 0.4324\n",
      "Epoch 00017: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 47s 336ms/step - loss: 1.0717 - accuracy: 0.4324 - val_loss: 1.0823 - val_accuracy: 0.4196 - lr: 8.0000e-06\n",
      "Epoch 18/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0832 - accuracy: 0.4099\n",
      "Epoch 00018: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 48s 344ms/step - loss: 1.0832 - accuracy: 0.4099 - val_loss: 1.0816 - val_accuracy: 0.4206 - lr: 8.0000e-06\n",
      "Epoch 19/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0757 - accuracy: 0.4235\n",
      "Epoch 00019: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "141/141 [==============================] - 48s 342ms/step - loss: 1.0757 - accuracy: 0.4235 - val_loss: 1.0813 - val_accuracy: 0.4211 - lr: 8.0000e-06\n",
      "Epoch 20/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0731 - accuracy: 0.4313\n",
      "Epoch 00020: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0731 - accuracy: 0.4313 - val_loss: 1.0819 - val_accuracy: 0.4211 - lr: 1.6000e-06\n",
      "Epoch 21/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0738 - accuracy: 0.4322\n",
      "Epoch 00021: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 50s 357ms/step - loss: 1.0738 - accuracy: 0.4322 - val_loss: 1.0807 - val_accuracy: 0.4221 - lr: 1.6000e-06\n",
      "Epoch 22/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0728 - accuracy: 0.4333\n",
      "Epoch 00022: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "141/141 [==============================] - 46s 329ms/step - loss: 1.0728 - accuracy: 0.4333 - val_loss: 1.0818 - val_accuracy: 0.4225 - lr: 1.6000e-06\n",
      "Epoch 23/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0653 - accuracy: 0.4477\n",
      "Epoch 00023: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 46s 328ms/step - loss: 1.0653 - accuracy: 0.4477 - val_loss: 1.0802 - val_accuracy: 0.4235 - lr: 3.2000e-07\n",
      "Epoch 24/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0697 - accuracy: 0.4410\n",
      "Epoch 00024: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 49s 349ms/step - loss: 1.0697 - accuracy: 0.4410 - val_loss: 1.0814 - val_accuracy: 0.4225 - lr: 3.2000e-07\n",
      "Epoch 25/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0734 - accuracy: 0.4284\n",
      "Epoch 00025: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "141/141 [==============================] - 47s 333ms/step - loss: 1.0734 - accuracy: 0.4284 - val_loss: 1.0821 - val_accuracy: 0.4206 - lr: 3.2000e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0745 - accuracy: 0.4351\n",
      "Epoch 00026: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 46s 324ms/step - loss: 1.0745 - accuracy: 0.4351 - val_loss: 1.0817 - val_accuracy: 0.4206 - lr: 6.4000e-08\n",
      "Epoch 27/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0712 - accuracy: 0.4379\n",
      "Epoch 00027: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 45s 322ms/step - loss: 1.0712 - accuracy: 0.4379 - val_loss: 1.0826 - val_accuracy: 0.4196 - lr: 6.4000e-08\n",
      "Epoch 28/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0729 - accuracy: 0.4342\n",
      "Epoch 00028: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "141/141 [==============================] - 47s 332ms/step - loss: 1.0729 - accuracy: 0.4342 - val_loss: 1.0816 - val_accuracy: 0.4211 - lr: 6.4000e-08\n",
      "Epoch 29/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0743 - accuracy: 0.4353\n",
      "Epoch 00029: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 46s 323ms/step - loss: 1.0743 - accuracy: 0.4353 - val_loss: 1.0814 - val_accuracy: 0.4206 - lr: 1.2800e-08\n",
      "Epoch 30/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0771 - accuracy: 0.4322\n",
      "Epoch 00030: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 46s 326ms/step - loss: 1.0771 - accuracy: 0.4322 - val_loss: 1.0805 - val_accuracy: 0.4230 - lr: 1.2800e-08\n",
      "Epoch 31/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0740 - accuracy: 0.4355\n",
      "Epoch 00031: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "141/141 [==============================] - 47s 334ms/step - loss: 1.0740 - accuracy: 0.4355 - val_loss: 1.0813 - val_accuracy: 0.4216 - lr: 1.2800e-08\n",
      "Epoch 32/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0741 - accuracy: 0.4242\n",
      "Epoch 00032: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 49s 345ms/step - loss: 1.0741 - accuracy: 0.4242 - val_loss: 1.0812 - val_accuracy: 0.4221 - lr: 2.5600e-09\n",
      "Epoch 33/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0726 - accuracy: 0.4291\n",
      "Epoch 00033: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 52s 369ms/step - loss: 1.0726 - accuracy: 0.4291 - val_loss: 1.0821 - val_accuracy: 0.4201 - lr: 2.5600e-09\n",
      "Epoch 34/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0739 - accuracy: 0.4331\n",
      "Epoch 00034: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "141/141 [==============================] - 59s 416ms/step - loss: 1.0739 - accuracy: 0.4331 - val_loss: 1.0810 - val_accuracy: 0.4235 - lr: 2.5600e-09\n",
      "Epoch 35/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0730 - accuracy: 0.4364\n",
      "Epoch 00035: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 52s 371ms/step - loss: 1.0730 - accuracy: 0.4364 - val_loss: 1.0800 - val_accuracy: 0.4240 - lr: 5.1200e-10\n",
      "Epoch 36/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0740 - accuracy: 0.4324\n",
      "Epoch 00036: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 49s 345ms/step - loss: 1.0740 - accuracy: 0.4324 - val_loss: 1.0811 - val_accuracy: 0.4216 - lr: 5.1200e-10\n",
      "Epoch 37/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0793 - accuracy: 0.4235\n",
      "Epoch 00037: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0793 - accuracy: 0.4235 - val_loss: 1.0816 - val_accuracy: 0.4201 - lr: 5.1200e-10\n",
      "Epoch 38/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0748 - accuracy: 0.4344\n",
      "Epoch 00038: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0748 - accuracy: 0.4344 - val_loss: 1.0817 - val_accuracy: 0.4216 - lr: 1.0240e-10\n",
      "Epoch 39/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0696 - accuracy: 0.4359\n",
      "Epoch 00039: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 49s 347ms/step - loss: 1.0696 - accuracy: 0.4359 - val_loss: 1.0807 - val_accuracy: 0.4230 - lr: 1.0240e-10\n",
      "Epoch 40/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0743 - accuracy: 0.4284\n",
      "Epoch 00040: val_loss did not improve from 1.07963\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
      "141/141 [==============================] - 47s 334ms/step - loss: 1.0743 - accuracy: 0.4284 - val_loss: 1.0807 - val_accuracy: 0.4221 - lr: 1.0240e-10\n",
      "Epoch 41/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0755 - accuracy: 0.4284\n",
      "Epoch 00041: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 48s 340ms/step - loss: 1.0755 - accuracy: 0.4284 - val_loss: 1.0810 - val_accuracy: 0.4235 - lr: 2.0480e-11\n",
      "Epoch 42/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0713 - accuracy: 0.4322\n",
      "Epoch 00042: val_loss did not improve from 1.07963\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0713 - accuracy: 0.4322 - val_loss: 1.0805 - val_accuracy: 0.4221 - lr: 2.0480e-11\n",
      "Epoch 43/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0812 - accuracy: 0.4184\n",
      "Epoch 00043: val_loss improved from 1.07963 to 1.07932, saving model to rong_test1_fer3.h5\n",
      "141/141 [==============================] - 50s 354ms/step - loss: 1.0812 - accuracy: 0.4184 - val_loss: 1.0793 - val_accuracy: 0.4255 - lr: 2.0480e-11\n",
      "Epoch 44/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0725 - accuracy: 0.4377\n",
      "Epoch 00044: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.0725 - accuracy: 0.4377 - val_loss: 1.0810 - val_accuracy: 0.4225 - lr: 2.0480e-11\n",
      "Epoch 45/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0723 - accuracy: 0.4366\n",
      "Epoch 00045: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0723 - accuracy: 0.4366 - val_loss: 1.0817 - val_accuracy: 0.4216 - lr: 2.0480e-11\n",
      "Epoch 46/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0779 - accuracy: 0.4200\n",
      "Epoch 00046: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0779 - accuracy: 0.4200 - val_loss: 1.0801 - val_accuracy: 0.4245 - lr: 2.0480e-11\n",
      "Epoch 47/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0696 - accuracy: 0.4386\n",
      "Epoch 00047: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 358ms/step - loss: 1.0696 - accuracy: 0.4386 - val_loss: 1.0809 - val_accuracy: 0.4221 - lr: 4.0960e-12\n",
      "Epoch 48/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0776 - accuracy: 0.4231\n",
      "Epoch 00048: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0776 - accuracy: 0.4231 - val_loss: 1.0818 - val_accuracy: 0.4216 - lr: 4.0960e-12\n",
      "Epoch 49/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0719 - accuracy: 0.4346\n",
      "Epoch 00049: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 8.192000897078167e-13.\n",
      "141/141 [==============================] - 50s 354ms/step - loss: 1.0719 - accuracy: 0.4346 - val_loss: 1.0810 - val_accuracy: 0.4221 - lr: 4.0960e-12\n",
      "Epoch 50/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0777 - accuracy: 0.4291\n",
      "Epoch 00050: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 351ms/step - loss: 1.0777 - accuracy: 0.4291 - val_loss: 1.0825 - val_accuracy: 0.4206 - lr: 8.1920e-13\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - ETA: 0s - loss: 1.0793 - accuracy: 0.4200\n",
      "Epoch 00051: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0793 - accuracy: 0.4200 - val_loss: 1.0823 - val_accuracy: 0.4201 - lr: 8.1920e-13\n",
      "Epoch 52/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0742 - accuracy: 0.4257\n",
      "Epoch 00052: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.6384001360475466e-13.\n",
      "141/141 [==============================] - 49s 345ms/step - loss: 1.0742 - accuracy: 0.4257 - val_loss: 1.0820 - val_accuracy: 0.4211 - lr: 8.1920e-13\n",
      "Epoch 53/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0762 - accuracy: 0.4209\n",
      "Epoch 00053: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 345ms/step - loss: 1.0762 - accuracy: 0.4209 - val_loss: 1.0837 - val_accuracy: 0.4181 - lr: 1.6384e-13\n",
      "Epoch 54/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0753 - accuracy: 0.4262\n",
      "Epoch 00054: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0753 - accuracy: 0.4262 - val_loss: 1.0815 - val_accuracy: 0.4221 - lr: 1.6384e-13\n",
      "Epoch 55/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0788 - accuracy: 0.4242\n",
      "Epoch 00055: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 3.2768002178849846e-14.\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0788 - accuracy: 0.4242 - val_loss: 1.0824 - val_accuracy: 0.4201 - lr: 1.6384e-13\n",
      "Epoch 56/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0732 - accuracy: 0.4320\n",
      "Epoch 00056: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 349ms/step - loss: 1.0732 - accuracy: 0.4320 - val_loss: 1.0829 - val_accuracy: 0.4191 - lr: 3.2768e-14\n",
      "Epoch 57/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0738 - accuracy: 0.4293\n",
      "Epoch 00057: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 48s 338ms/step - loss: 1.0738 - accuracy: 0.4293 - val_loss: 1.0802 - val_accuracy: 0.4235 - lr: 3.2768e-14\n",
      "Epoch 58/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0823 - accuracy: 0.4078\n",
      "Epoch 00058: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 6.553600300244697e-15.\n",
      "141/141 [==============================] - 51s 361ms/step - loss: 1.0823 - accuracy: 0.4078 - val_loss: 1.0825 - val_accuracy: 0.4206 - lr: 3.2768e-14\n",
      "Epoch 59/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0708 - accuracy: 0.4384\n",
      "Epoch 00059: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0708 - accuracy: 0.4384 - val_loss: 1.0816 - val_accuracy: 0.4206 - lr: 6.5536e-15\n",
      "Epoch 60/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0709 - accuracy: 0.4344\n",
      "Epoch 00060: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 347ms/step - loss: 1.0709 - accuracy: 0.4344 - val_loss: 1.0797 - val_accuracy: 0.4255 - lr: 6.5536e-15\n",
      "Epoch 61/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0704 - accuracy: 0.4366\n",
      "Epoch 00061: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.3107200431082805e-15.\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0704 - accuracy: 0.4366 - val_loss: 1.0802 - val_accuracy: 0.4250 - lr: 6.5536e-15\n",
      "Epoch 62/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0732 - accuracy: 0.4266\n",
      "Epoch 00062: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0732 - accuracy: 0.4266 - val_loss: 1.0813 - val_accuracy: 0.4230 - lr: 1.3107e-15\n",
      "Epoch 63/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0731 - accuracy: 0.4322\n",
      "Epoch 00063: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0731 - accuracy: 0.4322 - val_loss: 1.0810 - val_accuracy: 0.4221 - lr: 1.3107e-15\n",
      "Epoch 64/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0745 - accuracy: 0.4306\n",
      "Epoch 00064: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 2.6214401285682084e-16.\n",
      "141/141 [==============================] - 48s 343ms/step - loss: 1.0745 - accuracy: 0.4306 - val_loss: 1.0811 - val_accuracy: 0.4230 - lr: 1.3107e-15\n",
      "Epoch 65/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0716 - accuracy: 0.4353\n",
      "Epoch 00065: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 48s 344ms/step - loss: 1.0716 - accuracy: 0.4353 - val_loss: 1.0827 - val_accuracy: 0.4186 - lr: 2.6214e-16\n",
      "Epoch 66/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0750 - accuracy: 0.4331\n",
      "Epoch 00066: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0750 - accuracy: 0.4331 - val_loss: 1.0811 - val_accuracy: 0.4221 - lr: 2.6214e-16\n",
      "Epoch 67/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0788 - accuracy: 0.4161\n",
      "Epoch 00067: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 5.2428803630155353e-17.\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0788 - accuracy: 0.4161 - val_loss: 1.0819 - val_accuracy: 0.4211 - lr: 2.6214e-16\n",
      "Epoch 68/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0777 - accuracy: 0.4209\n",
      "Epoch 00068: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0777 - accuracy: 0.4209 - val_loss: 1.0824 - val_accuracy: 0.4196 - lr: 5.2429e-17\n",
      "Epoch 69/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0743 - accuracy: 0.4288\n",
      "Epoch 00069: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0743 - accuracy: 0.4288 - val_loss: 1.0826 - val_accuracy: 0.4196 - lr: 5.2429e-17\n",
      "Epoch 70/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0715 - accuracy: 0.4375\n",
      "Epoch 00070: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.0485760990728867e-17.\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0715 - accuracy: 0.4375 - val_loss: 1.0815 - val_accuracy: 0.4206 - lr: 5.2429e-17\n",
      "Epoch 71/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0676 - accuracy: 0.4419\n",
      "Epoch 00071: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0676 - accuracy: 0.4419 - val_loss: 1.0815 - val_accuracy: 0.4206 - lr: 1.0486e-17\n",
      "Epoch 72/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0780 - accuracy: 0.4242\n",
      "Epoch 00072: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0780 - accuracy: 0.4242 - val_loss: 1.0807 - val_accuracy: 0.4235 - lr: 1.0486e-17\n",
      "Epoch 73/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0778 - accuracy: 0.4251\n",
      "Epoch 00073: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 2.097152165058549e-18.\n",
      "141/141 [==============================] - 50s 351ms/step - loss: 1.0778 - accuracy: 0.4251 - val_loss: 1.0820 - val_accuracy: 0.4206 - lr: 1.0486e-17\n",
      "Epoch 74/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0780 - accuracy: 0.4273\n",
      "Epoch 00074: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0780 - accuracy: 0.4273 - val_loss: 1.0801 - val_accuracy: 0.4245 - lr: 2.0972e-18\n",
      "Epoch 75/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0759 - accuracy: 0.4258\n",
      "Epoch 00075: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0759 - accuracy: 0.4258 - val_loss: 1.0810 - val_accuracy: 0.4225 - lr: 2.0972e-18\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - ETA: 0s - loss: 1.0697 - accuracy: 0.4419\n",
      "Epoch 00076: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 4.19430449555322e-19.\n",
      "141/141 [==============================] - 48s 337ms/step - loss: 1.0697 - accuracy: 0.4419 - val_loss: 1.0816 - val_accuracy: 0.4216 - lr: 2.0972e-18\n",
      "Epoch 77/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0741 - accuracy: 0.4291\n",
      "Epoch 00077: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 48s 338ms/step - loss: 1.0741 - accuracy: 0.4291 - val_loss: 1.0797 - val_accuracy: 0.4250 - lr: 4.1943e-19\n",
      "Epoch 78/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0703 - accuracy: 0.4355\n",
      "Epoch 00078: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 48s 344ms/step - loss: 1.0703 - accuracy: 0.4355 - val_loss: 1.0823 - val_accuracy: 0.4201 - lr: 4.1943e-19\n",
      "Epoch 79/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0768 - accuracy: 0.4211\n",
      "Epoch 00079: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 8.388609197901593e-20.\n",
      "141/141 [==============================] - 49s 345ms/step - loss: 1.0768 - accuracy: 0.4211 - val_loss: 1.0795 - val_accuracy: 0.4255 - lr: 4.1943e-19\n",
      "Epoch 80/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.4204\n",
      "Epoch 00080: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0763 - accuracy: 0.4204 - val_loss: 1.0803 - val_accuracy: 0.4240 - lr: 8.3886e-20\n",
      "Epoch 81/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0685 - accuracy: 0.4446\n",
      "Epoch 00081: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0685 - accuracy: 0.4446 - val_loss: 1.0820 - val_accuracy: 0.4211 - lr: 8.3886e-20\n",
      "Epoch 82/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0753 - accuracy: 0.4282\n",
      "Epoch 00082: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.6777218395803187e-20.\n",
      "141/141 [==============================] - 49s 345ms/step - loss: 1.0753 - accuracy: 0.4282 - val_loss: 1.0820 - val_accuracy: 0.4201 - lr: 8.3886e-20\n",
      "Epoch 83/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0748 - accuracy: 0.4269\n",
      "Epoch 00083: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 47s 336ms/step - loss: 1.0748 - accuracy: 0.4269 - val_loss: 1.0803 - val_accuracy: 0.4235 - lr: 1.6777e-20\n",
      "Epoch 84/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0708 - accuracy: 0.4377\n",
      "Epoch 00084: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 46s 328ms/step - loss: 1.0708 - accuracy: 0.4377 - val_loss: 1.0811 - val_accuracy: 0.4221 - lr: 1.6777e-20\n",
      "Epoch 85/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0760 - accuracy: 0.4231\n",
      "Epoch 00085: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 3.3554436145371517e-21.\n",
      "141/141 [==============================] - 47s 337ms/step - loss: 1.0760 - accuracy: 0.4231 - val_loss: 1.0822 - val_accuracy: 0.4196 - lr: 1.6777e-20\n",
      "Epoch 86/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0699 - accuracy: 0.4384\n",
      "Epoch 00086: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 347ms/step - loss: 1.0699 - accuracy: 0.4384 - val_loss: 1.0817 - val_accuracy: 0.4206 - lr: 3.3554e-21\n",
      "Epoch 87/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0767 - accuracy: 0.4359\n",
      "Epoch 00087: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 48s 341ms/step - loss: 1.0767 - accuracy: 0.4359 - val_loss: 1.0816 - val_accuracy: 0.4216 - lr: 3.3554e-21\n",
      "Epoch 88/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0755 - accuracy: 0.4322\n",
      "Epoch 00088: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 6.710887229074304e-22.\n",
      "141/141 [==============================] - 48s 340ms/step - loss: 1.0755 - accuracy: 0.4322 - val_loss: 1.0809 - val_accuracy: 0.4221 - lr: 3.3554e-21\n",
      "Epoch 89/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0726 - accuracy: 0.4322\n",
      "Epoch 00089: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 48s 340ms/step - loss: 1.0726 - accuracy: 0.4322 - val_loss: 1.0820 - val_accuracy: 0.4206 - lr: 6.7109e-22\n",
      "Epoch 90/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0689 - accuracy: 0.4382\n",
      "Epoch 00090: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 48s 341ms/step - loss: 1.0689 - accuracy: 0.4382 - val_loss: 1.0813 - val_accuracy: 0.4221 - lr: 6.7109e-22\n",
      "Epoch 91/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0688 - accuracy: 0.4379\n",
      "Epoch 00091: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.3421774862045392e-22.\n",
      "141/141 [==============================] - 48s 338ms/step - loss: 1.0688 - accuracy: 0.4379 - val_loss: 1.0800 - val_accuracy: 0.4255 - lr: 6.7109e-22\n",
      "Epoch 92/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0751 - accuracy: 0.4282\n",
      "Epoch 00092: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 345ms/step - loss: 1.0751 - accuracy: 0.4282 - val_loss: 1.0813 - val_accuracy: 0.4216 - lr: 1.3422e-22\n",
      "Epoch 93/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0725 - accuracy: 0.4313\n",
      "Epoch 00093: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 48s 342ms/step - loss: 1.0725 - accuracy: 0.4313 - val_loss: 1.0821 - val_accuracy: 0.4211 - lr: 1.3422e-22\n",
      "Epoch 94/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0691 - accuracy: 0.4433\n",
      "Epoch 00094: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 2.684355073383274e-23.\n",
      "141/141 [==============================] - 54s 382ms/step - loss: 1.0691 - accuracy: 0.4433 - val_loss: 1.0794 - val_accuracy: 0.4260 - lr: 1.3422e-22\n",
      "Epoch 95/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0702 - accuracy: 0.4346\n",
      "Epoch 00095: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 351ms/step - loss: 1.0702 - accuracy: 0.4346 - val_loss: 1.0808 - val_accuracy: 0.4235 - lr: 2.6844e-23\n",
      "Epoch 96/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0700 - accuracy: 0.4328\n",
      "Epoch 00096: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 49s 351ms/step - loss: 1.0700 - accuracy: 0.4328 - val_loss: 1.0808 - val_accuracy: 0.4235 - lr: 2.6844e-23\n",
      "Epoch 97/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0734 - accuracy: 0.4304\n",
      "Epoch 00097: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 5.368710272984293e-24.\n",
      "141/141 [==============================] - 49s 345ms/step - loss: 1.0734 - accuracy: 0.4304 - val_loss: 1.0806 - val_accuracy: 0.4240 - lr: 2.6844e-23\n",
      "Epoch 98/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0741 - accuracy: 0.4264\n",
      "Epoch 00098: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0741 - accuracy: 0.4264 - val_loss: 1.0814 - val_accuracy: 0.4221 - lr: 5.3687e-24\n",
      "Epoch 99/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0710 - accuracy: 0.4295\n",
      "Epoch 00099: val_loss did not improve from 1.07932\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0710 - accuracy: 0.4295 - val_loss: 1.0810 - val_accuracy: 0.4230 - lr: 5.3687e-24\n",
      "Epoch 100/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0713 - accuracy: 0.4351\n",
      "Epoch 00100: val_loss did not improve from 1.07932\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.0737420861512948e-24.\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0713 - accuracy: 0.4351 - val_loss: 1.0828 - val_accuracy: 0.4211 - lr: 5.3687e-24\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples = 16944\n",
    "nb_validation_samples = 2099\n",
    "epochs=100\n",
    "\n",
    "history=model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(train_loss, label = 'Train Loss', color = 'gold')\n",
    "plt.plot(test_loss, label = 'Test Loss', color = 'orange')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_classifier = cv2.CascadeClassifier('../datasets/classifiers/haarcascade_frontalface_default.xml')\n",
    "# classifier =load_model('rong_test1_fer2.h5')\n",
    "\n",
    "# # class_labels = ['angry','disgust','happy','neutral','sad']\n",
    "# class_labels = ['angry','disgust','fear','happy','neutral','sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def face_detector(img):\n",
    "#     # Convert image to grayscale\n",
    "#     gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "#     if faces is ():\n",
    "#         return (0,0,0,0),np.zeros((48,48),np.uint8),img\n",
    "\n",
    "#     for (x,y,w,h) in faces:\n",
    "#         cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "#         roi_gray = gray[y:y+h,x:x+w]\n",
    "\n",
    "#     try:\n",
    "#         roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "#     except:\n",
    "#         return (x,w,y,h),np.zeros((48,48),np.uint8),img\n",
    "#     return (x,w,y,h),roi_gray,img\n",
    "\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "#     # Grab a single frame of video\n",
    "#     ret, frame = cap.read()\n",
    "#     labels = []\n",
    "#     gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "#     for (x,y,w,h) in faces:\n",
    "#         cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "#         roi_gray = gray[y:y+h,x:x+w]\n",
    "#         roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "#     # rect,face,image = face_detector(frame)\n",
    "\n",
    "\n",
    "#         if np.sum([roi_gray])!=0:\n",
    "#             roi = roi_gray.astype('float')/255.0\n",
    "#             roi = img_to_array(roi)\n",
    "#             roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "#         # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "#             preds = classifier.predict(roi)[0]\n",
    "#             label=class_labels[(preds.argmax())]\n",
    "#             label_position = (x,y)\n",
    "#             cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "#         else:\n",
    "#             cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "#     cv2.imshow('Emotion Detector',frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
