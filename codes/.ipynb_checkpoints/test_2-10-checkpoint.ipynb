{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credits: APEKSHA PRIYA, Priya Dwivedi\n",
    "\n",
    "https://github.com/priya-dwivedi/face_and_emotion_detection/blob/master/src/EmotionDetector_v2.ipynb\n",
    "https://towardsdatascience.com/face-detection-recognition-and-emotion-detection-in-8-lines-of-code-b2ce32d4d5de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing\n",
    "num_classes = 4\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 120 #(change back for full load)\n",
    "\n",
    "train_data_dir = '../test_full/train_cropped'\n",
    "validation_data_dir = '../test_full/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27800 images belonging to 4 classes.\n",
      "Found 2590 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Let's use some data augmentaiton \n",
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'happy': 1, 'neutral': 2, 'sad': 3}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the model\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3), activation='elu',input_shape=(48,48,1)))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size=(3, 3), activation='elu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(128, kernel_size=(3, 3), activation='elu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(256, kernel_size=(3, 3), activation='elu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size=(1, 1), activation='elu'))\n",
    "# # # model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(3, kernel_size=(4, 4), activation='elu'))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Activation(\"softmax\"))\n",
    "\n",
    "# model.summary()\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "# model.add(Conv2D(5, kernel_size=(4, 4), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "# model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               15480     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 484       \n",
      "=================================================================\n",
      "Total params: 1,125,084\n",
      "Trainable params: 1,125,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model 2\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=(48,48,1)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(2, 2), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "#model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "# model.add(Conv2D(5, kernel_size=(4, 4), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-51a94684a5b7>:49: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.3354 - accuracy: 0.3640\n",
      "Epoch 00001: val_loss improved from inf to 1.34955, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 145s 629ms/step - loss: 1.3354 - accuracy: 0.3640 - val_loss: 1.3496 - val_accuracy: 0.3639 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.2863 - accuracy: 0.4054\n",
      "Epoch 00002: val_loss improved from 1.34955 to 1.29580, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 148s 642ms/step - loss: 1.2863 - accuracy: 0.4054 - val_loss: 1.2958 - val_accuracy: 0.4151 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.2038 - accuracy: 0.4743\n",
      "Epoch 00003: val_loss improved from 1.29580 to 1.24797, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 149s 644ms/step - loss: 1.2038 - accuracy: 0.4743 - val_loss: 1.2480 - val_accuracy: 0.4321 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.1407 - accuracy: 0.5096\n",
      "Epoch 00004: val_loss improved from 1.24797 to 1.23736, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 148s 641ms/step - loss: 1.1407 - accuracy: 0.5096 - val_loss: 1.2374 - val_accuracy: 0.4516 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.1036 - accuracy: 0.5243\n",
      "Epoch 00005: val_loss improved from 1.23736 to 1.22436, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 150s 650ms/step - loss: 1.1036 - accuracy: 0.5243 - val_loss: 1.2244 - val_accuracy: 0.4651 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.0726 - accuracy: 0.5352\n",
      "Epoch 00006: val_loss improved from 1.22436 to 1.21132, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 150s 648ms/step - loss: 1.0726 - accuracy: 0.5352 - val_loss: 1.2113 - val_accuracy: 0.4659 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.0416 - accuracy: 0.5538\n",
      "Epoch 00007: val_loss did not improve from 1.21132\n",
      "231/231 [==============================] - 157s 679ms/step - loss: 1.0416 - accuracy: 0.5538 - val_loss: 1.2227 - val_accuracy: 0.4726 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.0144 - accuracy: 0.5655\n",
      "Epoch 00008: val_loss did not improve from 1.21132\n",
      "231/231 [==============================] - 149s 645ms/step - loss: 1.0144 - accuracy: 0.5655 - val_loss: 1.2232 - val_accuracy: 0.4829 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9893 - accuracy: 0.5774\n",
      "Epoch 00009: val_loss improved from 1.21132 to 1.16518, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 149s 645ms/step - loss: 0.9893 - accuracy: 0.5774 - val_loss: 1.1652 - val_accuracy: 0.4873 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9733 - accuracy: 0.5859\n",
      "Epoch 00010: val_loss improved from 1.16518 to 1.15638, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 150s 649ms/step - loss: 0.9733 - accuracy: 0.5859 - val_loss: 1.1564 - val_accuracy: 0.5012 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9545 - accuracy: 0.6016\n",
      "Epoch 00011: val_loss did not improve from 1.15638\n",
      "231/231 [==============================] - 151s 655ms/step - loss: 0.9545 - accuracy: 0.6016 - val_loss: 1.1628 - val_accuracy: 0.5111 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9315 - accuracy: 0.6117\n",
      "Epoch 00012: val_loss did not improve from 1.15638\n",
      "231/231 [==============================] - 163s 706ms/step - loss: 0.9315 - accuracy: 0.6117 - val_loss: 1.1916 - val_accuracy: 0.5183 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9144 - accuracy: 0.6204\n",
      "Epoch 00013: val_loss did not improve from 1.15638\n",
      "231/231 [==============================] - 163s 706ms/step - loss: 0.9144 - accuracy: 0.6204 - val_loss: 1.1844 - val_accuracy: 0.5262 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8987 - accuracy: 0.6232\n",
      "Epoch 00014: val_loss did not improve from 1.15638\n",
      "231/231 [==============================] - 154s 666ms/step - loss: 0.8987 - accuracy: 0.6232 - val_loss: 1.2396 - val_accuracy: 0.5218 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8839 - accuracy: 0.6341\n",
      "Epoch 00015: val_loss did not improve from 1.15638\n",
      "231/231 [==============================] - 161s 696ms/step - loss: 0.8839 - accuracy: 0.6341 - val_loss: 1.2036 - val_accuracy: 0.5345 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8744 - accuracy: 0.6348\n",
      "Epoch 00016: val_loss did not improve from 1.15638\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "231/231 [==============================] - 157s 681ms/step - loss: 0.8744 - accuracy: 0.6348 - val_loss: 1.1653 - val_accuracy: 0.5437 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8448 - accuracy: 0.6533\n",
      "Epoch 00017: val_loss improved from 1.15638 to 1.15313, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 150s 647ms/step - loss: 0.8448 - accuracy: 0.6533 - val_loss: 1.1531 - val_accuracy: 0.5548 - lr: 2.0000e-05\n",
      "Epoch 18/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8413 - accuracy: 0.6566\n",
      "Epoch 00018: val_loss did not improve from 1.15313\n",
      "231/231 [==============================] - 152s 656ms/step - loss: 0.8413 - accuracy: 0.6566 - val_loss: 1.1541 - val_accuracy: 0.5520 - lr: 2.0000e-05\n",
      "Epoch 19/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8373 - accuracy: 0.6560\n",
      "Epoch 00019: val_loss did not improve from 1.15313\n",
      "231/231 [==============================] - 158s 683ms/step - loss: 0.8373 - accuracy: 0.6560 - val_loss: 1.1536 - val_accuracy: 0.5476 - lr: 2.0000e-05\n",
      "Epoch 20/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8349 - accuracy: 0.6599\n",
      "Epoch 00020: val_loss improved from 1.15313 to 1.14998, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 157s 677ms/step - loss: 0.8349 - accuracy: 0.6599 - val_loss: 1.1500 - val_accuracy: 0.5512 - lr: 2.0000e-05\n",
      "Epoch 21/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8292 - accuracy: 0.6602\n",
      "Epoch 00021: val_loss improved from 1.14998 to 1.13431, saving model to rong_test_2_10.hdf5\n",
      "231/231 [==============================] - 154s 668ms/step - loss: 0.8292 - accuracy: 0.6602 - val_loss: 1.1343 - val_accuracy: 0.5516 - lr: 2.0000e-05\n",
      "Epoch 22/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8289 - accuracy: 0.6613\n",
      "Epoch 00022: val_loss did not improve from 1.13431\n",
      "231/231 [==============================] - 149s 646ms/step - loss: 0.8289 - accuracy: 0.6613 - val_loss: 1.1638 - val_accuracy: 0.5452 - lr: 2.0000e-05\n",
      "Epoch 23/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8208 - accuracy: 0.6652\n",
      "Epoch 00023: val_loss did not improve from 1.13431\n",
      "231/231 [==============================] - 154s 665ms/step - loss: 0.8208 - accuracy: 0.6652 - val_loss: 1.1532 - val_accuracy: 0.5516 - lr: 2.0000e-05\n",
      "Epoch 24/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8205 - accuracy: 0.6679\n",
      "Epoch 00024: val_loss did not improve from 1.13431\n",
      "231/231 [==============================] - 165s 716ms/step - loss: 0.8205 - accuracy: 0.6679 - val_loss: 1.1675 - val_accuracy: 0.5548 - lr: 2.0000e-05\n",
      "Epoch 25/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8203 - accuracy: 0.6625\n",
      "Epoch 00025: val_loss did not improve from 1.13431\n",
      "231/231 [==============================] - 161s 696ms/step - loss: 0.8203 - accuracy: 0.6625 - val_loss: 1.1559 - val_accuracy: 0.5437 - lr: 2.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8158 - accuracy: 0.6683\n",
      "Epoch 00026: val_loss did not improve from 1.13431\n",
      "231/231 [==============================] - 167s 724ms/step - loss: 0.8158 - accuracy: 0.6683 - val_loss: 1.1755 - val_accuracy: 0.5496 - lr: 2.0000e-05\n",
      "Epoch 27/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8171 - accuracy: 0.6689\n",
      "Epoch 00027: val_loss did not improve from 1.13431\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "231/231 [==============================] - 156s 677ms/step - loss: 0.8171 - accuracy: 0.6689 - val_loss: 1.1370 - val_accuracy: 0.5528 - lr: 2.0000e-05\n",
      "Epoch 28/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8094 - accuracy: 0.6714\n",
      "Epoch 00028: val_loss did not improve from 1.13431\n",
      "231/231 [==============================] - 160s 692ms/step - loss: 0.8094 - accuracy: 0.6714 - val_loss: 1.1571 - val_accuracy: 0.5488 - lr: 4.0000e-06\n",
      "Epoch 29/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8086 - accuracy: 0.6709\n",
      "Epoch 00029: val_loss did not improve from 1.13431\n",
      "231/231 [==============================] - 156s 674ms/step - loss: 0.8086 - accuracy: 0.6709 - val_loss: 1.1472 - val_accuracy: 0.5524 - lr: 4.0000e-06\n",
      "Epoch 30/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8119 - accuracy: 0.6706Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.13431\n",
      "231/231 [==============================] - 164s 710ms/step - loss: 0.8119 - accuracy: 0.6706 - val_loss: 1.1410 - val_accuracy: 0.5536 - lr: 4.0000e-06\n",
      "Epoch 00030: early stopping\n"
     ]
    }
   ],
   "source": [
    "# If you want to train the same model or try other models, go for this\n",
    "\n",
    "\n",
    "# filepath = os.path.join(\"./emotion_detector_models/model_v6_{epoch}.hdf5\")\n",
    "\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint('rong_test_2_9.hdf5',\n",
    "#                                              monitor='val_accuracy',\n",
    "#                                              verbose=1,\n",
    "#                                              save_best_only=True,\n",
    "#                                              mode='max')\n",
    "\n",
    "# callbacks = [checkpoint]\n",
    "\n",
    "checkpoint = ModelCheckpoint('rong_test_2_10.hdf5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=9,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=6,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "# callbacks = [checkpoint,reduce_lr]\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "\n",
    "\n",
    "# if mode == \"train\":\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "nb_train_samples = 27800\n",
    "nb_validation_samples = 2590\n",
    "epochs = 100\n",
    "\n",
    "model_info = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks = callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "# plot_model_history(model_info)\n",
    "model.save_weights('rong_test_2_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model_info.history.keys())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_info.history['loss'])\n",
    "plt.plot(model_info.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('rong_test_2_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_info = model.fit_generator(\n",
    "#             train_generator,\n",
    "#             steps_per_epoch=nb_train_samples // batch_size,\n",
    "#             epochs=epochs,\n",
    "#             callbacks = callbacks,\n",
    "#             validation_data=validation_generator,\n",
    "#             validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.plot(model_info.history['loss'])\n",
    "# plt.plot(model_info.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('rong_test_2.h5')\n",
    "# # history = model.fit_generator(\n",
    "# #     train_generator,\n",
    "# #     steps_per_epoch = nb_train_samples // batch_size,\n",
    "# #     epochs = epochs,\n",
    "# #     callbacks = callbacks,\n",
    "# #     validation_data = validation_generator,\n",
    "# #     validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import sklearn\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import numpy as np\n",
    "# # Found 28709 images belonging to 7 classes.\n",
    "# # Found 3589 images belonging to 7 classes.\n",
    "\n",
    "\n",
    "# # nb_train_samples = 28273\n",
    "# # nb_validation_samples = 3534\n",
    "# nb_train_samples = 140\n",
    "# nb_validation_samples = 70\n",
    "\n",
    "# # We need to recreate our validation generator with shuffle = false\n",
    "# validation_generator = val_datagen.flow_from_directory(\n",
    "#         validation_data_dir,\n",
    "#         color_mode = 'grayscale',\n",
    "#         target_size=(img_rows, img_cols),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical',\n",
    "#         shuffle=False)\n",
    "\n",
    "# class_labels = validation_generator.class_indices\n",
    "# class_labels = {v: k for k, v in class_labels.items()}\n",
    "# classes = list(class_labels.values())\n",
    "\n",
    "# #Confution Matrix and Classification Report\n",
    "# Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "# y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# print('Confusion Matrix')\n",
    "# print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "# print('Classification Report')\n",
    "# target_names = list(class_labels.values())\n",
    "# print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "# plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "# tick_marks = np.arange(len(classes))\n",
    "# _ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "# _ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# classifier = load_model('./emotion_detector_models/model_v3_71.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_generator = val_datagen.flow_from_directory(\n",
    "#         validation_data_dir,\n",
    "#         color_mode = 'grayscale',\n",
    "#         target_size=(img_rows, img_cols),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical',\n",
    "#         shuffle=False)\n",
    "\n",
    "# class_labels = validation_generator.class_indices\n",
    "# class_labels = {v: k for k, v in class_labels.items()}\n",
    "# classes = list(class_labels.values())\n",
    "# print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# from keras.optimizers import RMSprop, SGD, Adam\n",
    "# from keras.preprocessing import image\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "# import re\n",
    "\n",
    "# def draw_test(name, pred, im, true_label):\n",
    "#     BLACK = [0,0,0]\n",
    "#     expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "#     cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "#     cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "#     cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "# def getRandomImage(path, img_width, img_height):\n",
    "#     \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "#     folders = path\n",
    "#     random_directory = np.random.randint(0,len(folders))\n",
    "#     path_class = folders[random_directory]\n",
    "#     file_path = path\n",
    "#     file_names = [f for f in listdir(file_path)]\n",
    "#     random_file_index = np.random.randint(0,len(file_names))\n",
    "#     image_name = file_names[random_file_index]\n",
    "#     final_path = file_path + \"/\" + image_name\n",
    "#     return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# # dimensions of our images\n",
    "# img_width, img_height = 48, 48\n",
    "\n",
    "# # We use a very small learning rate \n",
    "# model.compile(loss = 'categorical_crossentropy',\n",
    "#               optimizer = RMSprop(lr = 0.001),\n",
    "#               metrics = ['accuracy'])\n",
    "\n",
    "# files = []\n",
    "# predictions = []\n",
    "# true_labels = []\n",
    "\n",
    "# # predicting images\n",
    "# for i in range(0, 10):\n",
    "#     path = '../non_load/fer2013/train' \n",
    "#     img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "#     files.append(final_path)\n",
    "#     true_labels.append(true_label)\n",
    "#     x = image.img_to_array(img)\n",
    "#     x = x * 1./255\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     images = np.vstack([x])\n",
    "#     classes = model.predict_classes(images, batch_size = 10)\n",
    "#     predictions.append(classes)\n",
    "    \n",
    "# for i in range(0, len(files)):\n",
    "#     image = cv2.imread((files[i]))\n",
    "#     image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "#     draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "#     cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-28936e0903ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# cap.release()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from time import sleep\n",
    "# from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# face_classifier = cv2.CascadeClassifier('../datasets/classifiers/haarcascade_frontalface_default.xml')\n",
    "# classifier =load_model('rong_test_2_1.hdf5')\n",
    "\n",
    "# class_labels = validation_generator.class_indices\n",
    "# class_labels = {v: k for k, v in class_labels.items()}\n",
    "# classes = list(class_labels.values())\n",
    "\n",
    "# def face_detector(img):\n",
    "#     # Convert image to grayscale\n",
    "#     gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "#     if faces is ():\n",
    "#         return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "#     for (x,y,w,h) in faces:\n",
    "#         cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "#         roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "#     try:\n",
    "#         roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "#     except:\n",
    "#         return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "#     return (x,w,y,h), roi_gray, img\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "\n",
    "#     ret, frame = cap.read()\n",
    "#     rect, face, image = face_detector(frame)\n",
    "#     if np.sum([face]) != 0.0:\n",
    "#         roi = face.astype(\"float\") / 255.0\n",
    "#         roi = img_to_array(roi)\n",
    "#         roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "#         # make a prediction on the ROI, then lookup the class\n",
    "#         preds = classifier.predict(roi)[0]\n",
    "#         label = class_labels[preds.argmax()]  \n",
    "#         label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "#         cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "#     else:\n",
    "#         cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "#     cv2.imshow('All', image)\n",
    "#     if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "#         break\n",
    "        \n",
    "# cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
