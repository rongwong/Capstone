{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credits: APEKSHA PRIYA, Priya Dwivedi\n",
    "\n",
    "https://github.com/priya-dwivedi/face_and_emotion_detection/blob/master/src/EmotionDetector_v2.ipynb\n",
    "https://towardsdatascience.com/face-detection-recognition-and-emotion-detection-in-8-lines-of-code-b2ce32d4d5de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing\n",
    "num_classes = 7\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 120 #(change back for full load)\n",
    "\n",
    "train_data_dir = '../non_load/fer2013/train'\n",
    "validation_data_dir = '../non_load/fer2013/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28643 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Let's use some data augmentaiton \n",
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 5)           645       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 5)           405       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 241,306\n",
      "Trainable params: 241,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=(48,48,1)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(5, kernel_size=(1, 1), activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(5, kernel_size=(4, 4), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "# model.add(Conv2D(5, kernel_size=(4, 4), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "# model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 44, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 7)           903       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 1, 1, 7)           791       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               960       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 847       \n",
      "=================================================================\n",
      "Total params: 243,757\n",
      "Trainable params: 243,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model 2\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=(48,48,1)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(7, kernel_size=(1, 1), activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(7, kernel_size=(4, 4), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "#model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "# model.add(Conv2D(5, kernel_size=(4, 4), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-d198eeab3a53>:48: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.9041 - accuracy: 0.2277\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.24540, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 130s 547ms/step - loss: 1.9041 - accuracy: 0.2277 - val_loss: 1.8557 - val_accuracy: 0.2454\n",
      "Epoch 2/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.8539 - accuracy: 0.2384\n",
      "Epoch 00002: val_accuracy did not improve from 0.24540\n",
      "238/238 [==============================] - 126s 529ms/step - loss: 1.8539 - accuracy: 0.2384 - val_loss: 1.8293 - val_accuracy: 0.2437\n",
      "Epoch 3/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.8378 - accuracy: 0.2418\n",
      "Epoch 00003: val_accuracy improved from 0.24540 to 0.24741, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 127s 535ms/step - loss: 1.8378 - accuracy: 0.2418 - val_loss: 1.8216 - val_accuracy: 0.2474\n",
      "Epoch 4/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.8273 - accuracy: 0.2437\n",
      "Epoch 00004: val_accuracy did not improve from 0.24741\n",
      "238/238 [==============================] - 133s 561ms/step - loss: 1.8273 - accuracy: 0.2437 - val_loss: 1.8115 - val_accuracy: 0.2454\n",
      "Epoch 5/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.8169 - accuracy: 0.2456\n",
      "Epoch 00005: val_accuracy did not improve from 0.24741\n",
      "238/238 [==============================] - 127s 535ms/step - loss: 1.8169 - accuracy: 0.2456 - val_loss: 1.7958 - val_accuracy: 0.2437\n",
      "Epoch 6/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.7994 - accuracy: 0.2527\n",
      "Epoch 00006: val_accuracy improved from 0.24741 to 0.29397, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 129s 544ms/step - loss: 1.7994 - accuracy: 0.2527 - val_loss: 1.7638 - val_accuracy: 0.2940\n",
      "Epoch 7/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.7768 - accuracy: 0.2696\n",
      "Epoch 00007: val_accuracy improved from 0.29397 to 0.29770, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 130s 546ms/step - loss: 1.7768 - accuracy: 0.2696 - val_loss: 1.7211 - val_accuracy: 0.2977\n",
      "Epoch 8/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.7536 - accuracy: 0.2877\n",
      "Epoch 00008: val_accuracy improved from 0.29770 to 0.31092, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 128s 536ms/step - loss: 1.7536 - accuracy: 0.2877 - val_loss: 1.6920 - val_accuracy: 0.3109\n",
      "Epoch 9/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.7376 - accuracy: 0.2961\n",
      "Epoch 00009: val_accuracy improved from 0.31092 to 0.31580, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 130s 547ms/step - loss: 1.7376 - accuracy: 0.2961 - val_loss: 1.6785 - val_accuracy: 0.3158\n",
      "Epoch 10/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.7185 - accuracy: 0.3051\n",
      "Epoch 00010: val_accuracy improved from 0.31580 to 0.32213, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 129s 543ms/step - loss: 1.7185 - accuracy: 0.3051 - val_loss: 1.6718 - val_accuracy: 0.3221\n",
      "Epoch 11/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.7042 - accuracy: 0.3132\n",
      "Epoch 00011: val_accuracy improved from 0.32213 to 0.33218, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 128s 539ms/step - loss: 1.7042 - accuracy: 0.3132 - val_loss: 1.6546 - val_accuracy: 0.3322\n",
      "Epoch 12/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.6909 - accuracy: 0.3172\n",
      "Epoch 00012: val_accuracy improved from 0.33218 to 0.34598, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 134s 565ms/step - loss: 1.6909 - accuracy: 0.3172 - val_loss: 1.6393 - val_accuracy: 0.3460\n",
      "Epoch 13/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.6771 - accuracy: 0.3257\n",
      "Epoch 00013: val_accuracy improved from 0.34598 to 0.34828, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 127s 535ms/step - loss: 1.6771 - accuracy: 0.3257 - val_loss: 1.6334 - val_accuracy: 0.3483\n",
      "Epoch 14/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.6677 - accuracy: 0.3334\n",
      "Epoch 00014: val_accuracy improved from 0.34828 to 0.35172, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 132s 556ms/step - loss: 1.6677 - accuracy: 0.3334 - val_loss: 1.6231 - val_accuracy: 0.3517\n",
      "Epoch 15/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.6505 - accuracy: 0.3364\n",
      "Epoch 00015: val_accuracy improved from 0.35172 to 0.36092, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 133s 559ms/step - loss: 1.6505 - accuracy: 0.3364 - val_loss: 1.6150 - val_accuracy: 0.3609\n",
      "Epoch 16/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.6420 - accuracy: 0.3426\n",
      "Epoch 00016: val_accuracy did not improve from 0.36092\n",
      "238/238 [==============================] - 133s 557ms/step - loss: 1.6420 - accuracy: 0.3426 - val_loss: 1.6103 - val_accuracy: 0.3534\n",
      "Epoch 17/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.6290 - accuracy: 0.3489\n",
      "Epoch 00017: val_accuracy improved from 0.36092 to 0.36207, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 76s 321ms/step - loss: 1.6290 - accuracy: 0.3489 - val_loss: 1.6077 - val_accuracy: 0.3621\n",
      "Epoch 18/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.6243 - accuracy: 0.3491\n",
      "Epoch 00018: val_accuracy did not improve from 0.36207\n",
      "238/238 [==============================] - 69s 288ms/step - loss: 1.6243 - accuracy: 0.3491 - val_loss: 1.6085 - val_accuracy: 0.3621\n",
      "Epoch 19/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.6153 - accuracy: 0.3560\n",
      "Epoch 00019: val_accuracy improved from 0.36207 to 0.36264, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 86s 363ms/step - loss: 1.6153 - accuracy: 0.3560 - val_loss: 1.5979 - val_accuracy: 0.3626\n",
      "Epoch 20/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5994 - accuracy: 0.3588\n",
      "Epoch 00020: val_accuracy improved from 0.36264 to 0.36868, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 72s 302ms/step - loss: 1.5994 - accuracy: 0.3588 - val_loss: 1.5958 - val_accuracy: 0.3687\n",
      "Epoch 21/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5987 - accuracy: 0.3670\n",
      "Epoch 00021: val_accuracy did not improve from 0.36868\n",
      "238/238 [==============================] - 83s 350ms/step - loss: 1.5987 - accuracy: 0.3670 - val_loss: 1.5954 - val_accuracy: 0.3658\n",
      "Epoch 22/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5898 - accuracy: 0.3678\n",
      "Epoch 00022: val_accuracy did not improve from 0.36868\n",
      "238/238 [==============================] - 85s 359ms/step - loss: 1.5898 - accuracy: 0.3678 - val_loss: 1.5969 - val_accuracy: 0.3678\n",
      "Epoch 23/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5795 - accuracy: 0.3773\n",
      "Epoch 00023: val_accuracy did not improve from 0.36868\n",
      "238/238 [==============================] - 74s 310ms/step - loss: 1.5795 - accuracy: 0.3773 - val_loss: 1.5787 - val_accuracy: 0.3687\n",
      "Epoch 24/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5738 - accuracy: 0.3753\n",
      "Epoch 00024: val_accuracy improved from 0.36868 to 0.38190, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 70s 294ms/step - loss: 1.5738 - accuracy: 0.3753 - val_loss: 1.5836 - val_accuracy: 0.3819\n",
      "Epoch 25/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5724 - accuracy: 0.3761\n",
      "Epoch 00025: val_accuracy did not improve from 0.38190\n",
      "238/238 [==============================] - 77s 323ms/step - loss: 1.5724 - accuracy: 0.3761 - val_loss: 1.5812 - val_accuracy: 0.3805\n",
      "Epoch 26/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5658 - accuracy: 0.3814\n",
      "Epoch 00026: val_accuracy did not improve from 0.38190\n",
      "238/238 [==============================] - 70s 296ms/step - loss: 1.5658 - accuracy: 0.3814 - val_loss: 1.5953 - val_accuracy: 0.3784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5554 - accuracy: 0.3841\n",
      "Epoch 00027: val_accuracy did not improve from 0.38190\n",
      "238/238 [==============================] - 72s 303ms/step - loss: 1.5554 - accuracy: 0.3841 - val_loss: 1.6126 - val_accuracy: 0.3718\n",
      "Epoch 28/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5552 - accuracy: 0.3846\n",
      "Epoch 00028: val_accuracy improved from 0.38190 to 0.38908, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 71s 300ms/step - loss: 1.5552 - accuracy: 0.3846 - val_loss: 1.5794 - val_accuracy: 0.3891\n",
      "Epoch 29/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5444 - accuracy: 0.3899\n",
      "Epoch 00029: val_accuracy did not improve from 0.38908\n",
      "238/238 [==============================] - 88s 370ms/step - loss: 1.5444 - accuracy: 0.3899 - val_loss: 1.5818 - val_accuracy: 0.3839\n",
      "Epoch 30/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5387 - accuracy: 0.3927\n",
      "Epoch 00030: val_accuracy improved from 0.38908 to 0.39023, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 75s 315ms/step - loss: 1.5387 - accuracy: 0.3927 - val_loss: 1.5729 - val_accuracy: 0.3902\n",
      "Epoch 31/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5315 - accuracy: 0.3956\n",
      "Epoch 00031: val_accuracy did not improve from 0.39023\n",
      "238/238 [==============================] - 70s 295ms/step - loss: 1.5315 - accuracy: 0.3956 - val_loss: 1.5737 - val_accuracy: 0.3865\n",
      "Epoch 32/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5306 - accuracy: 0.3967\n",
      "Epoch 00032: val_accuracy did not improve from 0.39023\n",
      "238/238 [==============================] - 69s 292ms/step - loss: 1.5306 - accuracy: 0.3967 - val_loss: 1.5628 - val_accuracy: 0.3859\n",
      "Epoch 33/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5233 - accuracy: 0.4043\n",
      "Epoch 00033: val_accuracy improved from 0.39023 to 0.39310, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 70s 293ms/step - loss: 1.5233 - accuracy: 0.4043 - val_loss: 1.5605 - val_accuracy: 0.3931\n",
      "Epoch 34/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5136 - accuracy: 0.4018\n",
      "Epoch 00034: val_accuracy improved from 0.39310 to 0.39971, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 73s 305ms/step - loss: 1.5136 - accuracy: 0.4018 - val_loss: 1.5538 - val_accuracy: 0.3997\n",
      "Epoch 35/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5041 - accuracy: 0.4090\n",
      "Epoch 00035: val_accuracy did not improve from 0.39971\n",
      "238/238 [==============================] - 75s 316ms/step - loss: 1.5041 - accuracy: 0.4090 - val_loss: 1.5518 - val_accuracy: 0.3977\n",
      "Epoch 36/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.5070 - accuracy: 0.4073\n",
      "Epoch 00036: val_accuracy did not improve from 0.39971\n",
      "238/238 [==============================] - 72s 301ms/step - loss: 1.5070 - accuracy: 0.4073 - val_loss: 1.5732 - val_accuracy: 0.3842\n",
      "Epoch 37/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4989 - accuracy: 0.4105\n",
      "Epoch 00037: val_accuracy improved from 0.39971 to 0.40431, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 72s 304ms/step - loss: 1.4989 - accuracy: 0.4105 - val_loss: 1.5570 - val_accuracy: 0.4043\n",
      "Epoch 38/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4937 - accuracy: 0.4164\n",
      "Epoch 00038: val_accuracy did not improve from 0.40431\n",
      "238/238 [==============================] - 76s 320ms/step - loss: 1.4937 - accuracy: 0.4164 - val_loss: 1.5747 - val_accuracy: 0.3983\n",
      "Epoch 39/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4938 - accuracy: 0.4143\n",
      "Epoch 00039: val_accuracy improved from 0.40431 to 0.40718, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 74s 313ms/step - loss: 1.4938 - accuracy: 0.4143 - val_loss: 1.5567 - val_accuracy: 0.4072\n",
      "Epoch 40/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4872 - accuracy: 0.4173\n",
      "Epoch 00040: val_accuracy did not improve from 0.40718\n",
      "238/238 [==============================] - 73s 306ms/step - loss: 1.4872 - accuracy: 0.4173 - val_loss: 1.5370 - val_accuracy: 0.4043\n",
      "Epoch 41/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4844 - accuracy: 0.4185\n",
      "Epoch 00041: val_accuracy improved from 0.40718 to 0.41351, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 72s 303ms/step - loss: 1.4844 - accuracy: 0.4185 - val_loss: 1.5381 - val_accuracy: 0.4135\n",
      "Epoch 42/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4755 - accuracy: 0.4206\n",
      "Epoch 00042: val_accuracy did not improve from 0.41351\n",
      "238/238 [==============================] - 76s 320ms/step - loss: 1.4755 - accuracy: 0.4206 - val_loss: 1.5325 - val_accuracy: 0.4075\n",
      "Epoch 43/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4725 - accuracy: 0.4245\n",
      "Epoch 00043: val_accuracy did not improve from 0.41351\n",
      "238/238 [==============================] - 74s 309ms/step - loss: 1.4725 - accuracy: 0.4245 - val_loss: 1.5419 - val_accuracy: 0.4060\n",
      "Epoch 44/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4670 - accuracy: 0.4241\n",
      "Epoch 00044: val_accuracy improved from 0.41351 to 0.41753, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 72s 302ms/step - loss: 1.4670 - accuracy: 0.4241 - val_loss: 1.5413 - val_accuracy: 0.4175\n",
      "Epoch 45/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4666 - accuracy: 0.4284\n",
      "Epoch 00045: val_accuracy did not improve from 0.41753\n",
      "238/238 [==============================] - 99s 415ms/step - loss: 1.4666 - accuracy: 0.4284 - val_loss: 1.5452 - val_accuracy: 0.4003\n",
      "Epoch 46/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4627 - accuracy: 0.4300\n",
      "Epoch 00046: val_accuracy improved from 0.41753 to 0.42471, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 90s 377ms/step - loss: 1.4627 - accuracy: 0.4300 - val_loss: 1.5354 - val_accuracy: 0.4247\n",
      "Epoch 47/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4551 - accuracy: 0.4285\n",
      "Epoch 00047: val_accuracy did not improve from 0.42471\n",
      "238/238 [==============================] - 74s 312ms/step - loss: 1.4551 - accuracy: 0.4285 - val_loss: 1.5389 - val_accuracy: 0.4078\n",
      "Epoch 48/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4573 - accuracy: 0.4304\n",
      "Epoch 00048: val_accuracy improved from 0.42471 to 0.42701, saving model to rong_test_2_1.hdf5\n",
      "238/238 [==============================] - 70s 295ms/step - loss: 1.4573 - accuracy: 0.4304 - val_loss: 1.5406 - val_accuracy: 0.4270\n",
      "Epoch 49/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4472 - accuracy: 0.4369\n",
      "Epoch 00049: val_accuracy did not improve from 0.42701\n",
      "238/238 [==============================] - 70s 295ms/step - loss: 1.4472 - accuracy: 0.4369 - val_loss: 1.5480 - val_accuracy: 0.4213\n",
      "Epoch 50/50\n",
      "238/238 [==============================] - ETA: 0s - loss: 1.4449 - accuracy: 0.4350\n",
      "Epoch 00050: val_accuracy did not improve from 0.42701\n",
      "238/238 [==============================] - 71s 299ms/step - loss: 1.4449 - accuracy: 0.4350 - val_loss: 1.5284 - val_accuracy: 0.4264\n"
     ]
    }
   ],
   "source": [
    "# If you want to train the same model or try other models, go for this\n",
    "\n",
    "\n",
    "# filepath = os.path.join(\"./emotion_detector_models/model_v6_{epoch}.hdf5\")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('rong_test_2_1.hdf5',\n",
    "                                             monitor='val_accuracy',\n",
    "                                             verbose=1,\n",
    "                                             save_best_only=True,\n",
    "                                             mode='max')\n",
    "\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# checkpoint = ModelCheckpoint('rong_test_2.hdf5',\n",
    "#                              monitor='val_loss',\n",
    "#                              mode='min',\n",
    "#                              save_best_only=True,\n",
    "#                              verbose=1)\n",
    "\n",
    "# earlystop = EarlyStopping(monitor='val_loss',\n",
    "#                           min_delta=0,\n",
    "#                           patience=9,\n",
    "#                           verbose=1,\n",
    "#                           restore_best_weights=True\n",
    "#                           )\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "#                               factor=0.2,\n",
    "#                               patience=3,\n",
    "#                               verbose=1,\n",
    "#                               min_delta=0.0001)\n",
    "\n",
    "# callbacks = [checkpoint,reduce_lr]\n",
    "# callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "\n",
    "\n",
    "# if mode == \"train\":\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "nb_train_samples = 28643\n",
    "nb_validation_samples = 3589\n",
    "epochs = 50\n",
    "model_info = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks = callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "# plot_model_history(model_info)\n",
    "model.save_weights('rong_test_2_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model_info.history.keys())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_info.history['loss'])\n",
    "plt.plot(model_info.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('rong_test_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_info = model.fit_generator(\n",
    "#             train_generator,\n",
    "#             steps_per_epoch=nb_train_samples // batch_size,\n",
    "#             epochs=epochs,\n",
    "#             callbacks = callbacks,\n",
    "#             validation_data=validation_generator,\n",
    "#             validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.plot(model_info.history['loss'])\n",
    "# plt.plot(model_info.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('rong_test_2.h5')\n",
    "# # history = model.fit_generator(\n",
    "# #     train_generator,\n",
    "# #     steps_per_epoch = nb_train_samples // batch_size,\n",
    "# #     epochs = epochs,\n",
    "# #     callbacks = callbacks,\n",
    "# #     validation_data = validation_generator,\n",
    "# #     validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import sklearn\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import numpy as np\n",
    "# # Found 28709 images belonging to 7 classes.\n",
    "# # Found 3589 images belonging to 7 classes.\n",
    "\n",
    "\n",
    "# # nb_train_samples = 28273\n",
    "# # nb_validation_samples = 3534\n",
    "# nb_train_samples = 140\n",
    "# nb_validation_samples = 70\n",
    "\n",
    "# # We need to recreate our validation generator with shuffle = false\n",
    "# validation_generator = val_datagen.flow_from_directory(\n",
    "#         validation_data_dir,\n",
    "#         color_mode = 'grayscale',\n",
    "#         target_size=(img_rows, img_cols),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical',\n",
    "#         shuffle=False)\n",
    "\n",
    "# class_labels = validation_generator.class_indices\n",
    "# class_labels = {v: k for k, v in class_labels.items()}\n",
    "# classes = list(class_labels.values())\n",
    "\n",
    "# #Confution Matrix and Classification Report\n",
    "# Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "# y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# print('Confusion Matrix')\n",
    "# print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "# print('Classification Report')\n",
    "# target_names = list(class_labels.values())\n",
    "# print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "# plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "# tick_marks = np.arange(len(classes))\n",
    "# _ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "# _ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# classifier = load_model('./emotion_detector_models/model_v3_71.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_generator = val_datagen.flow_from_directory(\n",
    "#         validation_data_dir,\n",
    "#         color_mode = 'grayscale',\n",
    "#         target_size=(img_rows, img_cols),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical',\n",
    "#         shuffle=False)\n",
    "\n",
    "# class_labels = validation_generator.class_indices\n",
    "# class_labels = {v: k for k, v in class_labels.items()}\n",
    "# classes = list(class_labels.values())\n",
    "# print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = path\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path\n",
    "    file_names = [f for f in listdir(file_path)]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = '../datasets/validate' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('../datasets/classifiers/haarcascade_frontalface_default.xml')\n",
    "classifier =load_model('rong_test_2.hdf5')\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
