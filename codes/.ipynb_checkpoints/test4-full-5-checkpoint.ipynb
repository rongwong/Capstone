{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.freecodecamp.org/news/facial-emotion-recognition-develop-a-c-n-n-and-break-into-kaggle-top-10-f618c024faa7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "from keras.layers import Conv2D, Activation, Dropout, MaxPooling2D, Flatten, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Data Loading Mechanism\n",
    "Now, we will define the load_data() function which will efficiently parse the data file and extract necessary data and then convert it into a usable image format.\n",
    "\n",
    "All the images in our dataset are 48x48 in dimension. Since these images are gray-scale, there is only one channel. We will extract the image data and rearrange it into a 48x48 array. Then convert it into unsigned integers and divide it by 255 to normalize the data. 255 is the maximum possible value of a single cell. By dividing every element by 255, we ensure that all our values range between 0 and 1.\n",
    "\n",
    "We will check the Usage column and store the data in separate lists, one for training the network and the other for testing it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(dataset_path):\n",
    "#     data = []\n",
    "#     test_data = []\n",
    "#     test_labels = []\n",
    "#     labels =[]\n",
    "#     with open(dataset_path, 'r') as file:\n",
    "#         for line_no, line in enumerate(file.readlines()):\n",
    "#             if 0 < line_no <= 35887:\n",
    "#             curr_class, line, set_type = line.split(',')\n",
    "#             image_data = np.asarray([int(x) for x in line.split()]).reshape(48, 48)\n",
    "#             image_data =image_data.astype(np.uint8)/255.0\n",
    "            \n",
    "#             if (set_type.strip() == 'PrivateTest'):\n",
    "              \n",
    "#               test_data.append(image_data)\n",
    "#               test_labels.append(curr_class)\n",
    "#             else:\n",
    "#               data.append(image_data)\n",
    "#               labels.append(curr_class)\n",
    "      \n",
    "#         test_data = np.expand_dims(test_data, -1)\n",
    "#         test_labels = to_categorical(test_labels, num_classes = 7)\n",
    "#         data = np.expand_dims(data, -1)   \n",
    "#         labels = to_categorical(labels, num_classes = 7)\n",
    "    \n",
    "#       return np.array(data), np.array(labels), np.array(test_data), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data is segregated, we will expand the dimensions of both testing and training data by one to accommodate the channel. Then, we will one hot encode all the labels using the to_categorical() function and return all the lists as numpy arrays.\n",
    "\n",
    "We will load the data by calling the load_data() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"/content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Data/fer2013.csv\"\n",
    "# train_data, train_labels, test_data, test_labels = load_data(dataset_path)\n",
    "# print(\"Number of images in Training set:\", len(train_data))\n",
    "# print(\"Number of images in Test set:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is loaded and now let us get to the best part, defining the network.\n",
    "\n",
    "Defining the model.\n",
    "We will use Keras to create a Sequential Convolutional Network. Which means that our neural network will be a linear stack of layers. This network will have the following components:\n",
    "\n",
    "Convolutional Layers: These layers are the building blocks of our network and these compute dot product between their weights and the small regions to which they are linked. This is how these layers learn certain features from these images.\n",
    "Activation functions: are those functions which are applied to the outputs of all layers in the network. In this project, we will resort to the use of two functionsâ€” Relu and Softmax.\n",
    "Pooling Layers: These layers will downsample the operation along the dimensions. This helps reduce the spatial data and minimize the processing power that is required.\n",
    "Dense layers: These layers are present at the end of a C.N.N. They take in all the feature data generated by the convolution layers and do the decision making.\n",
    "Dropout Layers: randomly turns off a few neurons in the network to prevent overfitting.\n",
    "Batch Normalization: normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. This speeds up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../test_full/train_cropped'\n",
    "validation_data_dir = '../test_full/validation'\n",
    "\n",
    "batch_size = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27800 images belonging to 4 classes.\n",
      "Found 2590 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Let's use some data augmentaiton \n",
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'happy': 1, 'neutral': 2, 'sad': 3}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing\n",
    "num_classes = 4\n",
    "img_rows, img_cols = 48, 48\n",
    "\n",
    "nb_train_samples = 27800\n",
    "nb_validation_samples = 2590\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model 2\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1), kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))    \n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))    \n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))    \n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))    \n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 23, 23, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 23, 23, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 23, 23, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 23, 23, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 23, 23, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 11, 11, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 11, 11, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 5, 5, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 9,010,692\n",
      "Trainable params: 9,007,108\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file= 'model.png')\n",
    "# model_plot = plt.imread('model.png')\n",
    "# plt.imshow(model_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compile the network using Adam optimizer and will use a variable learning rate. Since we are dealing with a classification problem that involves multiple categories, we will use categorical_crossentropy as our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback functions\n",
    "Callback functions are those functions which are called after every epoch during the training process. We will be using the following callback functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "adam = optimizers.Adam(lr = learning_rate)\n",
    "\n",
    "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=9, mode='auto')\n",
    "checkpointer = ModelCheckpoint('rong_test_4_5_checkpoint_file.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks=[checkpointer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train\n",
    "All our hard work is about to be put to the test. But before we fit the model, let us define some hyper-parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.4970 - accuracy: 0.3266\n",
      "Epoch 00001: val_loss improved from inf to 1.37172, saving model to rong_test_4_5_checkpoint_file.hd5\n",
      "WARNING:tensorflow:From /Users/rong/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: rong_test_4_5_checkpoint_file.hd5/assets\n",
      "868/868 [==============================] - 1089s 1s/step - loss: 1.4970 - accuracy: 0.3266 - val_loss: 1.3717 - val_accuracy: 0.3383\n",
      "Epoch 2/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3520 - accuracy: 0.3638\n",
      "Epoch 00002: val_loss improved from 1.37172 to 1.37010, saving model to rong_test_4_5_checkpoint_file.hd5\n",
      "INFO:tensorflow:Assets written to: rong_test_4_5_checkpoint_file.hd5/assets\n",
      "868/868 [==============================] - 1103s 1s/step - loss: 1.3520 - accuracy: 0.3638 - val_loss: 1.3701 - val_accuracy: 0.3387\n",
      "Epoch 3/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3452 - accuracy: 0.3641\n",
      "Epoch 00003: val_loss improved from 1.37010 to 1.36723, saving model to rong_test_4_5_checkpoint_file.hd5\n",
      "INFO:tensorflow:Assets written to: rong_test_4_5_checkpoint_file.hd5/assets\n",
      "868/868 [==============================] - 1005s 1s/step - loss: 1.3452 - accuracy: 0.3641 - val_loss: 1.3672 - val_accuracy: 0.3402\n",
      "Epoch 4/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3413 - accuracy: 0.3644\n",
      "Epoch 00004: val_loss improved from 1.36723 to 1.36703, saving model to rong_test_4_5_checkpoint_file.hd5\n",
      "INFO:tensorflow:Assets written to: rong_test_4_5_checkpoint_file.hd5/assets\n",
      "868/868 [==============================] - 1072s 1s/step - loss: 1.3413 - accuracy: 0.3644 - val_loss: 1.3670 - val_accuracy: 0.3387\n",
      "Epoch 5/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3436 - accuracy: 0.3644\n",
      "Epoch 00005: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 1058s 1s/step - loss: 1.3436 - accuracy: 0.3644 - val_loss: 1.3676 - val_accuracy: 0.3395\n",
      "Epoch 6/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3409 - accuracy: 0.3653\n",
      "Epoch 00006: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 1005s 1s/step - loss: 1.3409 - accuracy: 0.3653 - val_loss: 1.3711 - val_accuracy: 0.3387\n",
      "Epoch 7/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3414 - accuracy: 0.3657\n",
      "Epoch 00007: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 1185s 1s/step - loss: 1.3414 - accuracy: 0.3657 - val_loss: 1.3707 - val_accuracy: 0.3395\n",
      "Epoch 8/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3410 - accuracy: 0.3649\n",
      "Epoch 00008: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 1121s 1s/step - loss: 1.3410 - accuracy: 0.3649 - val_loss: 1.3705 - val_accuracy: 0.3395\n",
      "Epoch 9/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3409 - accuracy: 0.3653\n",
      "Epoch 00009: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 871s 1s/step - loss: 1.3409 - accuracy: 0.3653 - val_loss: 1.3695 - val_accuracy: 0.3406\n",
      "Epoch 10/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3409 - accuracy: 0.3656\n",
      "Epoch 00010: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 870s 1s/step - loss: 1.3409 - accuracy: 0.3656 - val_loss: 1.3703 - val_accuracy: 0.3395\n",
      "Epoch 11/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3437 - accuracy: 0.3656\n",
      "Epoch 00011: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1000ms/step - loss: 1.3437 - accuracy: 0.3656 - val_loss: 1.3712 - val_accuracy: 0.3387\n",
      "Epoch 12/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3416 - accuracy: 0.3656\n",
      "Epoch 00012: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 870s 1s/step - loss: 1.3416 - accuracy: 0.3656 - val_loss: 1.3708 - val_accuracy: 0.3402\n",
      "Epoch 13/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3438 - accuracy: 0.3655\n",
      "Epoch 00013: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1s/step - loss: 1.3438 - accuracy: 0.3655 - val_loss: 1.3715 - val_accuracy: 0.3398\n",
      "Epoch 14/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3403 - accuracy: 0.3657\n",
      "Epoch 00014: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1s/step - loss: 1.3403 - accuracy: 0.3657 - val_loss: 1.3701 - val_accuracy: 0.3414\n",
      "Epoch 15/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3395 - accuracy: 0.3658\n",
      "Epoch 00015: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1000ms/step - loss: 1.3395 - accuracy: 0.3658 - val_loss: 1.3709 - val_accuracy: 0.3391\n",
      "Epoch 16/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3398 - accuracy: 0.3659\n",
      "Epoch 00016: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 872s 1s/step - loss: 1.3398 - accuracy: 0.3659 - val_loss: 1.3701 - val_accuracy: 0.3402\n",
      "Epoch 17/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3406 - accuracy: 0.3656\n",
      "Epoch 00017: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 867s 999ms/step - loss: 1.3406 - accuracy: 0.3656 - val_loss: 1.3725 - val_accuracy: 0.3387\n",
      "Epoch 18/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3411 - accuracy: 0.3656\n",
      "Epoch 00018: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 866s 998ms/step - loss: 1.3411 - accuracy: 0.3656 - val_loss: 1.3718 - val_accuracy: 0.3383\n",
      "Epoch 19/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3396 - accuracy: 0.3657\n",
      "Epoch 00019: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 863s 995ms/step - loss: 1.3396 - accuracy: 0.3657 - val_loss: 1.3708 - val_accuracy: 0.3395\n",
      "Epoch 20/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3406 - accuracy: 0.3657\n",
      "Epoch 00020: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 864s 996ms/step - loss: 1.3406 - accuracy: 0.3657 - val_loss: 1.3713 - val_accuracy: 0.3395\n",
      "Epoch 21/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3407 - accuracy: 0.3659\n",
      "Epoch 00021: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 867s 999ms/step - loss: 1.3407 - accuracy: 0.3659 - val_loss: 1.3722 - val_accuracy: 0.3391\n",
      "Epoch 22/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3409 - accuracy: 0.3656\n",
      "Epoch 00022: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 866s 998ms/step - loss: 1.3409 - accuracy: 0.3656 - val_loss: 1.3706 - val_accuracy: 0.3395\n",
      "Epoch 23/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3396 - accuracy: 0.3657\n",
      "Epoch 00023: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 866s 997ms/step - loss: 1.3396 - accuracy: 0.3657 - val_loss: 1.3715 - val_accuracy: 0.3395\n",
      "Epoch 24/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3405 - accuracy: 0.3659\n",
      "Epoch 00024: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1000ms/step - loss: 1.3405 - accuracy: 0.3659 - val_loss: 1.3712 - val_accuracy: 0.3391\n",
      "Epoch 25/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3405 - accuracy: 0.3657\n",
      "Epoch 00025: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1s/step - loss: 1.3405 - accuracy: 0.3657 - val_loss: 1.3711 - val_accuracy: 0.3398\n",
      "Epoch 26/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3400 - accuracy: 0.3659\n",
      "Epoch 00026: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1s/step - loss: 1.3400 - accuracy: 0.3659 - val_loss: 1.3704 - val_accuracy: 0.3395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3405 - accuracy: 0.3658\n",
      "Epoch 00027: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1s/step - loss: 1.3405 - accuracy: 0.3658 - val_loss: 1.3705 - val_accuracy: 0.3406\n",
      "Epoch 28/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3396 - accuracy: 0.3658\n",
      "Epoch 00028: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1000ms/step - loss: 1.3396 - accuracy: 0.3658 - val_loss: 1.3701 - val_accuracy: 0.3395\n",
      "Epoch 29/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3658\n",
      "Epoch 00029: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 867s 999ms/step - loss: 1.3394 - accuracy: 0.3658 - val_loss: 1.3694 - val_accuracy: 0.3406\n",
      "Epoch 30/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3398 - accuracy: 0.3657\n",
      "Epoch 00030: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 864s 996ms/step - loss: 1.3398 - accuracy: 0.3657 - val_loss: 1.3701 - val_accuracy: 0.3395\n",
      "Epoch 31/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3659\n",
      "Epoch 00031: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 868s 1s/step - loss: 1.3393 - accuracy: 0.3659 - val_loss: 1.3712 - val_accuracy: 0.3387\n",
      "Epoch 32/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3659\n",
      "Epoch 00032: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 864s 995ms/step - loss: 1.3393 - accuracy: 0.3659 - val_loss: 1.3708 - val_accuracy: 0.3398\n",
      "Epoch 33/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3396 - accuracy: 0.3657\n",
      "Epoch 00033: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 866s 998ms/step - loss: 1.3396 - accuracy: 0.3657 - val_loss: 1.3703 - val_accuracy: 0.3398\n",
      "Epoch 34/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3657\n",
      "Epoch 00034: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 899s 1s/step - loss: 1.3394 - accuracy: 0.3657 - val_loss: 1.3702 - val_accuracy: 0.3402\n",
      "Epoch 35/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3397 - accuracy: 0.3659\n",
      "Epoch 00035: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 694s 799ms/step - loss: 1.3397 - accuracy: 0.3659 - val_loss: 1.3706 - val_accuracy: 0.3375\n",
      "Epoch 36/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3395 - accuracy: 0.3657\n",
      "Epoch 00036: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 473s 544ms/step - loss: 1.3395 - accuracy: 0.3657 - val_loss: 1.3718 - val_accuracy: 0.3375\n",
      "Epoch 37/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3659\n",
      "Epoch 00037: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 473s 545ms/step - loss: 1.3393 - accuracy: 0.3659 - val_loss: 1.3708 - val_accuracy: 0.3387\n",
      "Epoch 38/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3658\n",
      "Epoch 00038: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 471s 543ms/step - loss: 1.3392 - accuracy: 0.3658 - val_loss: 1.3698 - val_accuracy: 0.3406\n",
      "Epoch 39/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3403 - accuracy: 0.3659\n",
      "Epoch 00039: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 478s 551ms/step - loss: 1.3403 - accuracy: 0.3659 - val_loss: 1.3695 - val_accuracy: 0.3414\n",
      "Epoch 40/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3404 - accuracy: 0.3657\n",
      "Epoch 00040: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 476s 548ms/step - loss: 1.3404 - accuracy: 0.3657 - val_loss: 1.3700 - val_accuracy: 0.3395\n",
      "Epoch 41/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3400 - accuracy: 0.3659\n",
      "Epoch 00041: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 478s 551ms/step - loss: 1.3400 - accuracy: 0.3659 - val_loss: 1.3708 - val_accuracy: 0.3375\n",
      "Epoch 42/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3411 - accuracy: 0.3658\n",
      "Epoch 00042: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 478s 551ms/step - loss: 1.3411 - accuracy: 0.3658 - val_loss: 1.3705 - val_accuracy: 0.3402\n",
      "Epoch 43/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3658\n",
      "Epoch 00043: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 476s 548ms/step - loss: 1.3394 - accuracy: 0.3658 - val_loss: 1.3707 - val_accuracy: 0.3398\n",
      "Epoch 44/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3660\n",
      "Epoch 00044: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 474s 546ms/step - loss: 1.3394 - accuracy: 0.3660 - val_loss: 1.3714 - val_accuracy: 0.3387\n",
      "Epoch 45/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3395 - accuracy: 0.3658\n",
      "Epoch 00045: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 472s 543ms/step - loss: 1.3395 - accuracy: 0.3658 - val_loss: 1.3715 - val_accuracy: 0.3391\n",
      "Epoch 46/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3395 - accuracy: 0.3658\n",
      "Epoch 00046: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 473s 545ms/step - loss: 1.3395 - accuracy: 0.3658 - val_loss: 1.3701 - val_accuracy: 0.3406\n",
      "Epoch 47/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3395 - accuracy: 0.3659\n",
      "Epoch 00047: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 471s 543ms/step - loss: 1.3395 - accuracy: 0.3659 - val_loss: 1.3712 - val_accuracy: 0.3391\n",
      "Epoch 48/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3657\n",
      "Epoch 00048: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 484s 557ms/step - loss: 1.3394 - accuracy: 0.3657 - val_loss: 1.3688 - val_accuracy: 0.3398\n",
      "Epoch 49/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3658\n",
      "Epoch 00049: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 477s 550ms/step - loss: 1.3393 - accuracy: 0.3658 - val_loss: 1.3718 - val_accuracy: 0.3387\n",
      "Epoch 50/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3658\n",
      "Epoch 00050: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 477s 549ms/step - loss: 1.3392 - accuracy: 0.3658 - val_loss: 1.3698 - val_accuracy: 0.3383\n",
      "Epoch 51/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3657\n",
      "Epoch 00051: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 476s 548ms/step - loss: 1.3393 - accuracy: 0.3657 - val_loss: 1.3706 - val_accuracy: 0.3398\n",
      "Epoch 52/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3399 - accuracy: 0.3657\n",
      "Epoch 00052: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 477s 550ms/step - loss: 1.3399 - accuracy: 0.3657 - val_loss: 1.3702 - val_accuracy: 0.3395\n",
      "Epoch 53/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3397 - accuracy: 0.3658\n",
      "Epoch 00053: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 479s 552ms/step - loss: 1.3397 - accuracy: 0.3658 - val_loss: 1.3709 - val_accuracy: 0.3383\n",
      "Epoch 54/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3659\n",
      "Epoch 00054: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 474s 546ms/step - loss: 1.3392 - accuracy: 0.3659 - val_loss: 1.3701 - val_accuracy: 0.3398\n",
      "Epoch 55/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3659\n",
      "Epoch 00055: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 477s 550ms/step - loss: 1.3392 - accuracy: 0.3659 - val_loss: 1.3719 - val_accuracy: 0.3383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3659\n",
      "Epoch 00056: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 476s 549ms/step - loss: 1.3394 - accuracy: 0.3659 - val_loss: 1.3703 - val_accuracy: 0.3379\n",
      "Epoch 57/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3395 - accuracy: 0.3657\n",
      "Epoch 00057: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 477s 550ms/step - loss: 1.3395 - accuracy: 0.3657 - val_loss: 1.3708 - val_accuracy: 0.3395\n",
      "Epoch 58/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3657\n",
      "Epoch 00058: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 505s 582ms/step - loss: 1.3393 - accuracy: 0.3657 - val_loss: 1.3708 - val_accuracy: 0.3391\n",
      "Epoch 59/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3659\n",
      "Epoch 00059: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 538s 620ms/step - loss: 1.3394 - accuracy: 0.3659 - val_loss: 1.3717 - val_accuracy: 0.3387\n",
      "Epoch 60/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3395 - accuracy: 0.3657\n",
      "Epoch 00060: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 537s 618ms/step - loss: 1.3395 - accuracy: 0.3657 - val_loss: 1.3722 - val_accuracy: 0.3375\n",
      "Epoch 61/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3658\n",
      "Epoch 00061: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 531s 612ms/step - loss: 1.3392 - accuracy: 0.3658 - val_loss: 1.3704 - val_accuracy: 0.3387\n",
      "Epoch 62/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3391 - accuracy: 0.3660\n",
      "Epoch 00062: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 532s 613ms/step - loss: 1.3391 - accuracy: 0.3660 - val_loss: 1.3699 - val_accuracy: 0.3391\n",
      "Epoch 63/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3395 - accuracy: 0.3658\n",
      "Epoch 00063: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 531s 612ms/step - loss: 1.3395 - accuracy: 0.3658 - val_loss: 1.3706 - val_accuracy: 0.3391\n",
      "Epoch 64/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3398 - accuracy: 0.3659\n",
      "Epoch 00064: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 529s 610ms/step - loss: 1.3398 - accuracy: 0.3659 - val_loss: 1.3693 - val_accuracy: 0.3406\n",
      "Epoch 65/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3659\n",
      "Epoch 00065: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 537s 619ms/step - loss: 1.3392 - accuracy: 0.3659 - val_loss: 1.3715 - val_accuracy: 0.3383\n",
      "Epoch 66/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3658\n",
      "Epoch 00066: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 596s 686ms/step - loss: 1.3393 - accuracy: 0.3658 - val_loss: 1.3708 - val_accuracy: 0.3398\n",
      "Epoch 67/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3659\n",
      "Epoch 00067: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 579s 667ms/step - loss: 1.3394 - accuracy: 0.3659 - val_loss: 1.3714 - val_accuracy: 0.3383\n",
      "Epoch 68/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3395 - accuracy: 0.3657\n",
      "Epoch 00068: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 578s 665ms/step - loss: 1.3395 - accuracy: 0.3657 - val_loss: 1.3692 - val_accuracy: 0.3414\n",
      "Epoch 69/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3658\n",
      "Epoch 00069: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 594s 684ms/step - loss: 1.3392 - accuracy: 0.3658 - val_loss: 1.3712 - val_accuracy: 0.3395\n",
      "Epoch 70/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3396 - accuracy: 0.3657\n",
      "Epoch 00070: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 589s 678ms/step - loss: 1.3396 - accuracy: 0.3657 - val_loss: 1.3711 - val_accuracy: 0.3395\n",
      "Epoch 71/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3659\n",
      "Epoch 00071: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 585s 674ms/step - loss: 1.3392 - accuracy: 0.3659 - val_loss: 1.3703 - val_accuracy: 0.3387\n",
      "Epoch 72/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3659\n",
      "Epoch 00072: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 569s 656ms/step - loss: 1.3392 - accuracy: 0.3659 - val_loss: 1.3699 - val_accuracy: 0.3387\n",
      "Epoch 73/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3659\n",
      "Epoch 00073: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 570s 657ms/step - loss: 1.3393 - accuracy: 0.3659 - val_loss: 1.3712 - val_accuracy: 0.3395\n",
      "Epoch 74/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3657\n",
      "Epoch 00074: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 571s 657ms/step - loss: 1.3394 - accuracy: 0.3657 - val_loss: 1.3701 - val_accuracy: 0.3406\n",
      "Epoch 75/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3660\n",
      "Epoch 00075: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 557s 642ms/step - loss: 1.3392 - accuracy: 0.3660 - val_loss: 1.3709 - val_accuracy: 0.3398\n",
      "Epoch 76/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3658\n",
      "Epoch 00076: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 550s 634ms/step - loss: 1.3392 - accuracy: 0.3658 - val_loss: 1.3713 - val_accuracy: 0.3391\n",
      "Epoch 77/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3660\n",
      "Epoch 00077: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 546s 629ms/step - loss: 1.3392 - accuracy: 0.3660 - val_loss: 1.3706 - val_accuracy: 0.3398\n",
      "Epoch 78/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3656\n",
      "Epoch 00078: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 550s 634ms/step - loss: 1.3394 - accuracy: 0.3656 - val_loss: 1.3708 - val_accuracy: 0.3383\n",
      "Epoch 79/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3659\n",
      "Epoch 00079: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 547s 630ms/step - loss: 1.3392 - accuracy: 0.3659 - val_loss: 1.3708 - val_accuracy: 0.3379\n",
      "Epoch 80/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3658\n",
      "Epoch 00080: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 569s 656ms/step - loss: 1.3393 - accuracy: 0.3658 - val_loss: 1.3698 - val_accuracy: 0.3406\n",
      "Epoch 81/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3658\n",
      "Epoch 00081: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 588s 677ms/step - loss: 1.3392 - accuracy: 0.3658 - val_loss: 1.3717 - val_accuracy: 0.3391\n",
      "Epoch 82/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3402 - accuracy: 0.3656\n",
      "Epoch 00082: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 603s 694ms/step - loss: 1.3402 - accuracy: 0.3656 - val_loss: 1.3709 - val_accuracy: 0.3402\n",
      "Epoch 83/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3659\n",
      "Epoch 00083: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 708s 816ms/step - loss: 1.3393 - accuracy: 0.3659 - val_loss: 1.3707 - val_accuracy: 0.3402\n",
      "Epoch 84/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3659\n",
      "Epoch 00084: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 571s 658ms/step - loss: 1.3394 - accuracy: 0.3659 - val_loss: 1.3715 - val_accuracy: 0.3383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3400 - accuracy: 0.3658\n",
      "Epoch 00085: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 586s 676ms/step - loss: 1.3400 - accuracy: 0.3658 - val_loss: 1.3713 - val_accuracy: 0.3383\n",
      "Epoch 86/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3393 - accuracy: 0.3658\n",
      "Epoch 00086: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 584s 673ms/step - loss: 1.3393 - accuracy: 0.3658 - val_loss: 1.3688 - val_accuracy: 0.3422\n",
      "Epoch 87/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3658\n",
      "Epoch 00087: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 698s 804ms/step - loss: 1.3394 - accuracy: 0.3658 - val_loss: 1.3700 - val_accuracy: 0.3395\n",
      "Epoch 88/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3394 - accuracy: 0.3656\n",
      "Epoch 00088: val_loss did not improve from 1.36703\n",
      "868/868 [==============================] - 599s 690ms/step - loss: 1.3394 - accuracy: 0.3656 - val_loss: 1.3712 - val_accuracy: 0.3379\n",
      "Epoch 89/100\n",
      "868/868 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.3660"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# epochs = 100\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# model.fit(train_data,\n",
    "#           train_labels,\n",
    "#           epochs = epochs,\n",
    "#           batch_size = batch_size,\n",
    "#           validation_split = 0.2,\n",
    "#           shuffle = True,\n",
    "#           callbacks=[lr_reducer, checkpointer, early_stopper]\n",
    "#          )\n",
    "\n",
    "model_info = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks = callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"rong_test_4_5_model_file_json.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights('rong_test_4_5_weights.h5')\n",
    "model.save('rong_test_4_5_model_file.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_info.history.keys())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_info.history['loss'])\n",
    "plt.plot(model_info.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the network is being trained, I suggest that you go and finish that book you started or go for a run. It took me about an hour on Google Colab.\n",
    "\n",
    "Test the model\n",
    "Remember the private set we stored separately? That was for this very moment. This is the moment of truth and this is where we will reap the fruit of our labor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_test_labels = np.argmax(model.predict(test_data), axis=1)\n",
    "# test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# print (\"Accuracy score = \", accuracy_score(test_labels, predicted_test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_json = model.to_json()\n",
    "# with open(\"rong_test_4_2_model_file_json.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "# model.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/FERmodel.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('rong_test_4_2_weights.h5')\n",
    "# model.save('rong_test_4_2_model_file.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
