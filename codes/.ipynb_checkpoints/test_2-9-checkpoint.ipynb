{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credits: APEKSHA PRIYA, Priya Dwivedi\n",
    "\n",
    "https://github.com/priya-dwivedi/face_and_emotion_detection/blob/master/src/EmotionDetector_v2.ipynb\n",
    "https://towardsdatascience.com/face-detection-recognition-and-emotion-detection-in-8-lines-of-code-b2ce32d4d5de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing\n",
    "num_classes = 4\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 120 #(change back for full load)\n",
    "\n",
    "train_data_dir = '../test_full/train_cropped'\n",
    "validation_data_dir = '../test_full/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27800 images belonging to 4 classes.\n",
      "Found 2590 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Let's use some data augmentaiton \n",
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'happy': 1, 'neutral': 2, 'sad': 3}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the model\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3), activation='elu',input_shape=(48,48,1)))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size=(3, 3), activation='elu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(128, kernel_size=(3, 3), activation='elu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(256, kernel_size=(3, 3), activation='elu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size=(1, 1), activation='elu'))\n",
    "# # # model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(3, kernel_size=(4, 4), activation='elu'))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Activation(\"softmax\"))\n",
    "\n",
    "# model.summary()\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "# model.add(Conv2D(5, kernel_size=(4, 4), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "# model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               15480     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 484       \n",
      "=================================================================\n",
      "Total params: 1,125,084\n",
      "Trainable params: 1,125,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model 2\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='elu',input_shape=(48,48,1)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='elu'))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(2, 2), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "#model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "# model.add(Conv2D(5, kernel_size=(4, 4), activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-e179325b9330>:49: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.2813 - accuracy: 0.4174\n",
      "Epoch 00001: val_loss improved from inf to 1.30761, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 85s 368ms/step - loss: 1.2813 - accuracy: 0.4174 - val_loss: 1.3076 - val_accuracy: 0.4286 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.1798 - accuracy: 0.4970\n",
      "Epoch 00002: val_loss improved from 1.30761 to 1.26343, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 85s 368ms/step - loss: 1.1798 - accuracy: 0.4970 - val_loss: 1.2634 - val_accuracy: 0.4429 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.1208 - accuracy: 0.5199\n",
      "Epoch 00003: val_loss improved from 1.26343 to 1.24830, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 94s 407ms/step - loss: 1.1208 - accuracy: 0.5199 - val_loss: 1.2483 - val_accuracy: 0.4488 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.0827 - accuracy: 0.5384\n",
      "Epoch 00004: val_loss improved from 1.24830 to 1.23060, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 158s 686ms/step - loss: 1.0827 - accuracy: 0.5384 - val_loss: 1.2306 - val_accuracy: 0.4655 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.0442 - accuracy: 0.5617\n",
      "Epoch 00005: val_loss did not improve from 1.23060\n",
      "231/231 [==============================] - 161s 699ms/step - loss: 1.0442 - accuracy: 0.5617 - val_loss: 1.2313 - val_accuracy: 0.4857 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.0192 - accuracy: 0.5727\n",
      "Epoch 00006: val_loss improved from 1.23060 to 1.19415, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 160s 694ms/step - loss: 1.0192 - accuracy: 0.5727 - val_loss: 1.1941 - val_accuracy: 0.4976 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 1.0010 - accuracy: 0.5817\n",
      "Epoch 00007: val_loss did not improve from 1.19415\n",
      "231/231 [==============================] - 163s 704ms/step - loss: 1.0010 - accuracy: 0.5817 - val_loss: 1.2292 - val_accuracy: 0.5087 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9747 - accuracy: 0.5939\n",
      "Epoch 00008: val_loss did not improve from 1.19415\n",
      "231/231 [==============================] - 159s 690ms/step - loss: 0.9747 - accuracy: 0.5939 - val_loss: 1.1989 - val_accuracy: 0.4968 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9627 - accuracy: 0.5987\n",
      "Epoch 00009: val_loss did not improve from 1.19415\n",
      "231/231 [==============================] - 170s 738ms/step - loss: 0.9627 - accuracy: 0.5987 - val_loss: 1.2142 - val_accuracy: 0.5226 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9522 - accuracy: 0.6045\n",
      "Epoch 00010: val_loss improved from 1.19415 to 1.17669, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 163s 707ms/step - loss: 0.9522 - accuracy: 0.6045 - val_loss: 1.1767 - val_accuracy: 0.5310 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9354 - accuracy: 0.6121\n",
      "Epoch 00011: val_loss improved from 1.17669 to 1.15482, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 162s 702ms/step - loss: 0.9354 - accuracy: 0.6121 - val_loss: 1.1548 - val_accuracy: 0.5238 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.6153\n",
      "Epoch 00012: val_loss did not improve from 1.15482\n",
      "231/231 [==============================] - 163s 708ms/step - loss: 0.9214 - accuracy: 0.6153 - val_loss: 1.1756 - val_accuracy: 0.5389 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9136 - accuracy: 0.6231\n",
      "Epoch 00013: val_loss did not improve from 1.15482\n",
      "231/231 [==============================] - 166s 719ms/step - loss: 0.9136 - accuracy: 0.6231 - val_loss: 1.1776 - val_accuracy: 0.5361 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9070 - accuracy: 0.6225\n",
      "Epoch 00014: val_loss did not improve from 1.15482\n",
      "231/231 [==============================] - 178s 769ms/step - loss: 0.9070 - accuracy: 0.6225 - val_loss: 1.1800 - val_accuracy: 0.5250 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.9002 - accuracy: 0.6283\n",
      "Epoch 00015: val_loss improved from 1.15482 to 1.13721, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 178s 772ms/step - loss: 0.9002 - accuracy: 0.6283 - val_loss: 1.1372 - val_accuracy: 0.5512 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8938 - accuracy: 0.6332\n",
      "Epoch 00016: val_loss did not improve from 1.13721\n",
      "231/231 [==============================] - 170s 735ms/step - loss: 0.8938 - accuracy: 0.6332 - val_loss: 1.1516 - val_accuracy: 0.5452 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8822 - accuracy: 0.6378\n",
      "Epoch 00017: val_loss improved from 1.13721 to 1.13671, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 176s 762ms/step - loss: 0.8822 - accuracy: 0.6378 - val_loss: 1.1367 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8773 - accuracy: 0.6366\n",
      "Epoch 00018: val_loss did not improve from 1.13671\n",
      "231/231 [==============================] - 169s 731ms/step - loss: 0.8773 - accuracy: 0.6366 - val_loss: 1.1401 - val_accuracy: 0.5460 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8733 - accuracy: 0.6423\n",
      "Epoch 00019: val_loss did not improve from 1.13671\n",
      "231/231 [==============================] - 163s 705ms/step - loss: 0.8733 - accuracy: 0.6423 - val_loss: 1.1746 - val_accuracy: 0.5480 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8642 - accuracy: 0.6499\n",
      "Epoch 00020: val_loss did not improve from 1.13671\n",
      "231/231 [==============================] - 176s 761ms/step - loss: 0.8642 - accuracy: 0.6499 - val_loss: 1.1496 - val_accuracy: 0.5421 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8580 - accuracy: 0.6519\n",
      "Epoch 00021: val_loss did not improve from 1.13671\n",
      "231/231 [==============================] - 170s 737ms/step - loss: 0.8580 - accuracy: 0.6519 - val_loss: 1.1928 - val_accuracy: 0.5381 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8474 - accuracy: 0.6514\n",
      "Epoch 00022: val_loss improved from 1.13671 to 1.13331, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 172s 744ms/step - loss: 0.8474 - accuracy: 0.6514 - val_loss: 1.1333 - val_accuracy: 0.5440 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8477 - accuracy: 0.6541\n",
      "Epoch 00023: val_loss did not improve from 1.13331\n",
      "231/231 [==============================] - 164s 709ms/step - loss: 0.8477 - accuracy: 0.6541 - val_loss: 1.1450 - val_accuracy: 0.5627 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8372 - accuracy: 0.6584\n",
      "Epoch 00024: val_loss did not improve from 1.13331\n",
      "231/231 [==============================] - 169s 731ms/step - loss: 0.8372 - accuracy: 0.6584 - val_loss: 1.1560 - val_accuracy: 0.5659 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8366 - accuracy: 0.6581\n",
      "Epoch 00025: val_loss improved from 1.13331 to 1.12832, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 182s 787ms/step - loss: 0.8366 - accuracy: 0.6581 - val_loss: 1.1283 - val_accuracy: 0.5532 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8286 - accuracy: 0.6621\n",
      "Epoch 00026: val_loss did not improve from 1.12832\n",
      "231/231 [==============================] - 179s 775ms/step - loss: 0.8286 - accuracy: 0.6621 - val_loss: 1.2015 - val_accuracy: 0.5341 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8255 - accuracy: 0.6645\n",
      "Epoch 00027: val_loss did not improve from 1.12832\n",
      "231/231 [==============================] - 181s 782ms/step - loss: 0.8255 - accuracy: 0.6645 - val_loss: 1.1489 - val_accuracy: 0.5381 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8173 - accuracy: 0.6676\n",
      "Epoch 00028: val_loss did not improve from 1.12832\n",
      "231/231 [==============================] - 175s 759ms/step - loss: 0.8173 - accuracy: 0.6676 - val_loss: 1.1740 - val_accuracy: 0.5448 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8173 - accuracy: 0.6678\n",
      "Epoch 00029: val_loss did not improve from 1.12832\n",
      "231/231 [==============================] - 173s 747ms/step - loss: 0.8173 - accuracy: 0.6678 - val_loss: 1.1681 - val_accuracy: 0.5758 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8183 - accuracy: 0.6697\n",
      "Epoch 00030: val_loss did not improve from 1.12832\n",
      "231/231 [==============================] - 180s 777ms/step - loss: 0.8183 - accuracy: 0.6697 - val_loss: 1.2121 - val_accuracy: 0.5448 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.8045 - accuracy: 0.6716\n",
      "Epoch 00031: val_loss did not improve from 1.12832\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "231/231 [==============================] - 121s 523ms/step - loss: 0.8045 - accuracy: 0.6716 - val_loss: 1.1706 - val_accuracy: 0.5706 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7834 - accuracy: 0.6830\n",
      "Epoch 00032: val_loss did not improve from 1.12832\n",
      "231/231 [==============================] - 92s 397ms/step - loss: 0.7834 - accuracy: 0.6830 - val_loss: 1.1500 - val_accuracy: 0.5675 - lr: 2.0000e-05\n",
      "Epoch 33/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7833 - accuracy: 0.6822\n",
      "Epoch 00033: val_loss did not improve from 1.12832\n",
      "231/231 [==============================] - 96s 414ms/step - loss: 0.7833 - accuracy: 0.6822 - val_loss: 1.1600 - val_accuracy: 0.5552 - lr: 2.0000e-05\n",
      "Epoch 34/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7811 - accuracy: 0.6867\n",
      "Epoch 00034: val_loss improved from 1.12832 to 1.11972, saving model to rong_test_2_9.hdf5\n",
      "231/231 [==============================] - 99s 428ms/step - loss: 0.7811 - accuracy: 0.6867 - val_loss: 1.1197 - val_accuracy: 0.5770 - lr: 2.0000e-05\n",
      "Epoch 35/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7730 - accuracy: 0.6899\n",
      "Epoch 00035: val_loss did not improve from 1.11972\n",
      "231/231 [==============================] - 93s 401ms/step - loss: 0.7730 - accuracy: 0.6899 - val_loss: 1.1242 - val_accuracy: 0.5738 - lr: 2.0000e-05\n",
      "Epoch 36/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7679 - accuracy: 0.6914\n",
      "Epoch 00036: val_loss did not improve from 1.11972\n",
      "231/231 [==============================] - 92s 400ms/step - loss: 0.7679 - accuracy: 0.6914 - val_loss: 1.1627 - val_accuracy: 0.5663 - lr: 2.0000e-05\n",
      "Epoch 37/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7695 - accuracy: 0.6890\n",
      "Epoch 00037: val_loss did not improve from 1.11972\n",
      "231/231 [==============================] - 96s 416ms/step - loss: 0.7695 - accuracy: 0.6890 - val_loss: 1.1492 - val_accuracy: 0.5706 - lr: 2.0000e-05\n",
      "Epoch 38/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7657 - accuracy: 0.6912\n",
      "Epoch 00038: val_loss did not improve from 1.11972\n",
      "231/231 [==============================] - 96s 414ms/step - loss: 0.7657 - accuracy: 0.6912 - val_loss: 1.1211 - val_accuracy: 0.5770 - lr: 2.0000e-05\n",
      "Epoch 39/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7657 - accuracy: 0.6926\n",
      "Epoch 00039: val_loss did not improve from 1.11972\n",
      "231/231 [==============================] - 93s 404ms/step - loss: 0.7657 - accuracy: 0.6926 - val_loss: 1.1266 - val_accuracy: 0.5746 - lr: 2.0000e-05\n",
      "Epoch 40/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7690 - accuracy: 0.6922\n",
      "Epoch 00040: val_loss did not improve from 1.11972\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "231/231 [==============================] - 91s 393ms/step - loss: 0.7690 - accuracy: 0.6922 - val_loss: 1.1524 - val_accuracy: 0.5643 - lr: 2.0000e-05\n",
      "Epoch 41/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7575 - accuracy: 0.6935\n",
      "Epoch 00041: val_loss did not improve from 1.11972\n",
      "231/231 [==============================] - 97s 421ms/step - loss: 0.7575 - accuracy: 0.6935 - val_loss: 1.1401 - val_accuracy: 0.5726 - lr: 4.0000e-06\n",
      "Epoch 42/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7594 - accuracy: 0.6960\n",
      "Epoch 00042: val_loss did not improve from 1.11972\n",
      "231/231 [==============================] - 93s 404ms/step - loss: 0.7594 - accuracy: 0.6960 - val_loss: 1.1423 - val_accuracy: 0.5726 - lr: 4.0000e-06\n",
      "Epoch 43/100\n",
      "231/231 [==============================] - ETA: 0s - loss: 0.7625 - accuracy: 0.6923Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.11972\n",
      "231/231 [==============================] - 93s 404ms/step - loss: 0.7625 - accuracy: 0.6923 - val_loss: 1.1419 - val_accuracy: 0.5754 - lr: 4.0000e-06\n",
      "Epoch 00043: early stopping\n"
     ]
    }
   ],
   "source": [
    "# If you want to train the same model or try other models, go for this\n",
    "\n",
    "\n",
    "# filepath = os.path.join(\"./emotion_detector_models/model_v6_{epoch}.hdf5\")\n",
    "\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint('rong_test_2_9.hdf5',\n",
    "#                                              monitor='val_accuracy',\n",
    "#                                              verbose=1,\n",
    "#                                              save_best_only=True,\n",
    "#                                              mode='max')\n",
    "\n",
    "# callbacks = [checkpoint]\n",
    "\n",
    "checkpoint = ModelCheckpoint('rong_test_2_9.hdf5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=9,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=6,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "# callbacks = [checkpoint,reduce_lr]\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "\n",
    "\n",
    "# if mode == \"train\":\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "nb_train_samples = 27800\n",
    "nb_validation_samples = 2590\n",
    "epochs = 100\n",
    "\n",
    "model_info = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks = callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "# plot_model_history(model_info)\n",
    "model.save_weights('rong_test_2_9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3xV9fnA8c9zszdkEELC3nsvAUWQIag4sSq2VStSq3X/xNYO21pttdYtYsUtlbrqZhUBAYGAyN4ghJHByCLz5vv743uDIWTchNzcJPd5v155Jfeec8957klyn/PdYoxBKaWU73J4OwCllFLepYlAKaV8nCYCpZTycZoIlFLKx2kiUEopH6eJQCmlfJwmAqXcJCKvi8hf3Nx3v4hcdK7HUao+aCJQSikfp4lAKaV8nCYC1aS4qmQeEJGNIpIrIq+KSLyIfCki2SKySESal9n/MhHZIiInReRrEeleZlt/EVnvet17QHC5c10iIhtcr10pIn1qGfOtIrJbRI6LyCci0sr1vIjIP0UkTUSyRGSTiPRybZskIltdsR0SkftrdcGUQhOBapquAsYBXYBLgS+B3wBx2L/5XwOISBdgLnC3a9sXwKciEigigcDHwFtANPAf13FxvbY/MAe4DYgBXgY+EZGgmgQqImOAx4CpQALwA/Bv1+bxwPmu9xHl2ueYa9urwG3GmAigF/C/mpxXqbI0Eaim6DljTKox5hCwHFhtjPnOGJMPfAT0d+13LfC5MWahMaYIeBIIAc4DhgEBwNPGmCJjzPvA2jLnmA68bIxZbYxxGmPeAApcr6uJG4A5xpj1xpgC4CFguIi0A4qACKAbIMaYbcaYI67XFQE9RCTSGHPCGLO+hudV6jRNBKopSi3zc14Fj8NdP7fC3oEDYIwpAQ4Cia5th8yZszL+UObntsB9rmqhkyJyEmjtel1NlI8hB3vXn2iM+R/wPPACkCYis0Uk0rXrVcAk4AcRWSoiw2t4XqVO00SgfNlh7Ac6YOvksR/mh4AjQKLruVJtyvx8EHjUGNOszFeoMWbuOcYQhq1qOgRgjHnWGDMQ6IGtInrA9fxaY8wUoAW2CmteDc+r1GmaCJQvmwdMFpGxIhIA3Iet3lkJrAKKgV+LSICIXAkMKfPaV4AZIjLU1agbJiKTRSSihjHMBW4SkX6u9oW/Yquy9ovIYNfxA4BcIB8ocbVh3CAiUa4qrSyg5Byug/JxmgiUzzLG7ACmAc8BGdiG5UuNMYXGmELgSuDnwHFse8KHZV6bDNyKrbo5Aex27VvTGBYBvwM+wJZCOgI/cW2OxCacE9jqo2PAE65tNwL7RSQLmIFta1CqVkQXplFKKd+mJQKllPJxmgiUUsrHaSJQSikfp4lAKaV8nL+3A6ip2NhY065dO2+HoZRSjcq6desyjDFxFW1rdImgXbt2JCcnezsMpZRqVETkh8q2adWQUkr5OE0ESinl4zQRKKWUj2t0bQQVKSoqIiUlhfz8fG+H4nHBwcEkJSUREBDg7VCUUk1Ek0gEKSkpRERE0K5dO86cLLJpMcZw7NgxUlJSaN++vbfDUUo1EU2iaig/P5+YmJgmnQQARISYmBifKPkopepPk0gEQJNPAqV85X0qpepPk0kE1SrKh8wUMDptu1JKleWxRCAic0QkTUQ2V7J9iohsFJENIpIsIiM9FQsAzgLITYe8k3V+6JMnT/Liiy/W+HWTJk3i5Mm6j0cppWrCkyWC14GJVWxfDPQ1xvQDbgb+5cFYICgS/IMhJw3qeA2GyhJBcXFxla/74osvaNasWZ3GopRSNeWxRGCMWYZd2amy7TllFgYPAzy7Qo4IhLeA4jwoyK7TQ8+cOZM9e/bQr18/Bg8ezKhRo7jsssvo0aMHAJdffjkDBw6kZ8+ezJ49+/Tr2rVrR0ZGBvv376d79+7ceuut9OzZk/Hjx5OXl1enMSqlVGW82n1URK4AHsMuwD25iv2mA9MB2rRpU9luADzy6Ra2Hs6qfIfCXJATEBDidpw9WkXyh0t7Vrr98ccfZ/PmzWzYsIGvv/6ayZMns3nz5tNdPOfMmUN0dDR5eXkMHjyYq666ipiYmDOOsWvXLubOncsrr7zC1KlT+eCDD5g2bZrbMSqlVG15tbHYGPORMaYbcDnw5yr2m22MGWSMGRQXV+Hkee7zCwDj9Gij8ZAhQ87o5//ss8/St29fhg0bxsGDB9m1a9dZr2nfvj39+vUDYODAgezfv99j8SmlVFkNYkCZMWaZiHQQkVhjTMa5HKuqO3cASoohdQsER0HzdudyqkqFhYWd/vnrr79m0aJFrFq1itDQUEaPHl3hOICgoKDTP/v5+WnVkFKq3nitRCAincTVKV5EBgBBwDGPn9jhD6GxkHcCigvr5JARERFkZ1fc7pCZmUnz5s0JDQ1l+/btfPvtt3VyTqWUqiseKxGIyFxgNBArIinAH4AAAGPMLOAq4KciUgTkAdeWaTz2rLA425U0Nw2iks75cDExMYwYMYJevXoREhJCfHz86W0TJ05k1qxZdO/ena5duzJs2LBzPp9SStUlqa/P3royaNAgU35hmm3bttG9e/dqX1vkLMHfIXZ07on9kJ8J8T1tKaERcff9KqVUKRFZZ4wZVNE2nxlZfOJUIduOZFFY7GokDm9hG4xzz6lJQimlGj2fSQTB/vat5hU57RMBoRAYYauIdNoJpZQP85lEEBTgh4j8mAjAlgpKiuHUCe8FppRSXuYzicAhQnCAg7zCMokgKAL8Q2yjcSNrK1FKqbriM4kAICTAj7wiJ6cbyE9PO5EPBVWMRlZKqSbM5xKBs8RQ6CzTJhDSDBwBdjI6pZTyQb6VCAL9AM6sHhKHLRUU5kB+7UoFtZ2GGuDpp5/m1KlTtXqtUkrVBZ9KBMEVNRgDhMaAXyAc31uraao1ESilGrPGNZLqHDlECPYv12AM4PCDuK5w8gBkHYKCHGjWBvzcuzxlp6EeN24cLVq0YN68eRQUFHDFFVfwyCOPkJuby9SpU0lJScHpdPK73/2O1NRUDh8+zIUXXkhsbCxLlizxwLtWSqmqNb1E8OVMOLqp0s1tip0UlxhMoB9C+fV/DTiLwFloG5L9g0H8oGVvuPjxSo9ZdhrqBQsW8P7777NmzRqMMVx22WUsW7aM9PR0WrVqxeeffw7YOYiioqJ46qmnWLJkCbGxsXXx7pVSqsZ8qmoIbKnAmMpqf8RWEZWuVVCUZ5NCDdbMWbBgAQsWLKB///4MGDCA7du3s2vXLnr37s3ChQt58MEHWb58OVFRUXXxdpRS6pw1vRJBFXfuAEWFxexNy6FtdChRoYGV71hSDCcOQEEmBEXZx27MSWSM4aGHHuK22247a9v69ev54osvePjhhxk7diy///3vqz2eUkp5ms+VCIL9bZXQqfINxuU5/CG6PUQm2jEG6TuhuKDCXctOQz1hwgTmzJlDTk4OAIcOHSItLY3Dhw8TGhrKtGnTeOCBB1i/fv1Zr1VKKW9oeiWCajgcFYwwrkzpgLOAUNujKGMnRHeAwLAzdis7DfXFF1/M9ddfz/DhwwEIDw/n7bffZvfu3TzwwAM4HA4CAgJ46aWXAJg+fToTJ06kVatW2lislPIKn5qGulTK8VNk5hfRIyES19o41SvKh+N7wFlsVzYL8V4dv05DrZSqKZ2GupyQQDvCuMhZg1lHA4Ihtov9fmKvnbVUKaWaAJ9NBMDZA8uq4xcAMZ0gKBIyU+yYg0ZWolJKqfKaTCKoSRVXaYOxW+0E5Tn8bDtBaKwdhXxiP5SUK1kYY9c4KCmGklqcowqNrSpPKdXwNYnG4uDgYI4dO0ZMTIxbdf4OhxAU4CCvqJYL0ojYtY79AiH7MKTlAAK4EsAZC92IHbVcOjbBXcUFdjBbmdHNxhiOHTtGcHBw7eJWSqkKNIlEkJSUREpKCunp7tfbn8gtJL/ISX56DT+gyysqgaJM+7MI4LA5AbGP87PgUBaEx7l/zBInZB+xpY/wlq7jWsHBwSQlJZ1bzEopVUaTSAQBAQG0b9++Rq95c9V+fv/fLayYOYbEZueYDKqy4ln46nfws0+h/fnuveaze2Hda7Zkcf4DMOZhz8WnlPJ5TaaNoKZ6Jdrun5tSMj17oiHTITIJFv7+7LaEihzdbJPA4Fuh7/Ww/Ck4/J1nY1RK+TSfTQQ9EiLxcwhbDns4EQQEw5jf2g/zrR9Vva8xMP8hCI6C0TNh4l/tgLaPb4fiQs/GqZTyWT6bCIID/OjcIpxNhzycCAD6XAstesLiP1X9gb79c9i3DC78LYRGQ0hzuPQZSNsKy57wfJxKKZ/ks4kAbPXQ5kOZnu+S6fCDcY/YrqbrXqt4n+ICWPBbiOsOA2/68fkuE6DvdbD8H3B4Q9XnMQZWPg+zRkHW4ToLX6kzZB3xdgSqjvl0IuidGEVGTiFHs/I9f7JOF0G7UbD0bxUvifntizZRTHzs7AVxJj4GYXHw319VXqLIOwn/vsEmk6Mb4evH6vwtKMXBNfBUd9gw19uRqDrk04mgV2IkUA8NxmC7gI77E5w6BiufPXNbdiosexK6ToKOF5792pDmcOnTkLrZlgzKO/wdvHw+7JoPEx6Dob+E796G9B2eeS/Kd33zNGBg2d/tvFuqSfBYIhCROSKSJiKbK9l+g4hsFJFNIrJSRPp6KpbK9EiIwiGwuT7aCQASB0DPK2HVC5B99MfnF//JVg2N/0vlr+16sW1rWP4kHNlonzMG1v4LXh1vRzHf9CUMv912OQ0Is8dVqq6k74Qdn0PSYDsb75ZqOj+oRsOTJYLXgYlVbN8HXGCM6Q38GZjtwVgqFBLoR6cW4Ww+XEFVjaeM/Z1dDrO06ubwd7DhHRj2S4jpWPVrJz4OIdG2F9Gp4/DBL+Dz+6D9BXDbcmg9xO4XFgMj74Ltn9mivGqajIHdi+zfU31Y9ZxdvvUn79q2rOVPutclWjV4HksExphlwPEqtq80xpxwPfwW8Mpw2V6JUfXTc6hUdAcYdDOsf8tW3Xw5E8Ji7V18dUKjXVVEm+CZvrDlQxj7e7h+nv3wL2vY7RAeDwv/oBPjNVUH18DbV8GaeriHyk6F7/8N/a63XZrPvx/St8P2Tz1/buVxDaWN4Bbgy8o2ish0EUkWkeSaTCPhjt6JUaRnF5BaHw3GpS74P7vYzdtXw8Fv7Yd5cKR7r+02GfpNs4vj/PQTGHUfOCr4NQaGwQUPwoGVsHN+3cavGobdi+z39W95PtmvedmWPIbfYR/3vAKiO9puzXqj0eh5PRGIyIXYRPBgZfsYY2YbYwYZYwbFxdVgzh439K6vEcZlhcXCiLsg8wC07AP9bqjZ66c8D/dsgfajqt5vwE/tP+uiP9b5LKg+qaQEju3xdhQ/2rPYTkyYvg0Or/fceQpybFtU90t+rL50+NmbkKOb9EajCfBqIhCRPsC/gCnGmGPeiKFHq0hEqN/qIbCNugN+ClNesP9UNSHi3mv8AmybRPo22Phe7eJUP1rxNDw/uGEkg1PH4dB6GDoD/ENsLzFPWf8m5GfCeXed+XyfqdCsjZYKmgCvJQIRaQN8CNxojNnprThCA/3pGBdefz2HSgWGwWXPQUIfz56nx+XQagD871G73KaqnaI829vLOG3bjLft/Row0PNy6DEFNr0Phafq/jzOIjvGpc1waD34zG1+ATDyHjiU7IpHNVae7D46F1gFdBWRFBG5RURmiMgM1y6/B2KAF0Vkg4gkV3owD+udGMVmT8855C0idlRzVgqsfcXb0TRe370NpzIgrAVsbgDdJvf8D4KibJLvPw0Ksmwvsbq25WPIPGirMivS7waISNApUBo5T/Yaus4Yk2CMCTDGJBljXjXGzDLGzHJt/4Uxprkxpp/rq8JFletDr8QoUrMKSMtuonfM7c+HjmPtoLW8k96OpvFxFsPK52z/+fPvh7Qt3h2sZ4xNBB0usKPQ246A5u3gu7fq/jwrn7FrdXeeUPE+/kE2SfywAvavqNvzq3rj9cbihqC0wbjeq4fq00V/hPyTsOIZz56nMNd+SO5aBMmv2UFtn9xpp89orLZ+DCd/gBF322oYBDZ7sXooY6ddL7vjGPvY4bA9yfYtq9vrvPdr2xh83p0V90wrNeBndgqU5U/W3blVvWoSC9Ocq16JkQT6O1i2M4Mx3eK9HY5nJPSB3lPh25dg8C8gKrFmrzfGrpqWfdR+5Ry1fctzXI+zj8DJg5BXbuiI+AHGtk9c1Qirpoyx0yrEdrFTgDgc0G6kbScYPfOM1ePqze7F9ntpIgDodx0seRQ2vAsX/qZuzrPiGTsWpc+1Ve8XGGq7lS76A6Ssg6SBdXN+VW+0RIBtML6waxxfbDqCs6QJ934Y81v7/d/XVTzxXWWKC+Df19vJxl650L7+s3tg6eOw7TPIPGTrzntMsWMirnwFbvrKdnF9OM0Obtv8QcMpFSz9O2z9xL19dy+2A/hG3PXjXXHPy+1dedpWz8VYlT3/g5hO0Lztj89FJdnE8N077nUVPrTe9gbKPFTx9iMbYe8SV6+koOqPN/gWOyeWthU0SloicLm0byvmb0llzb7jDO8YU/0LGqPm7WDqm/aD/N/Xww3v24VzquIsgv/cBDu+gFH32/mSwltCREs7wtQvoPrzDr/Djn5d+RxMrmDSvPq0c4G9c3YEwM8/gzbDqt5/xdMQ0cqWpkp1nwJfPGCrh+J7ejbe8oryYf83tutxef2nwfs3wb6lZ5YWyju2B9663HYJBYjvDV3GQ+fxth3E4Wd/V4HhdhS8O4IibMJf8qhNIp7uDafqlJYIXMZ0a0FooB+fbWzi8/h3GQ+XvwT7l8MHt1Q9g6Sz2M5ntONzuPgJOyah22Rb9I9KdC8JAEQmQN+f2J43OWl18z5qo7gQvpppB9lFJcF7N1Z+RwyQkmyv03l3gH/gj8+Hx9kG+C0f1n//+YPfQnFexR/03Sbbu/KqxhQUZNvpysUBN34MFz1iR7V/8zTMmQBPdIT3b7YluAE/g5Bm7sc2ZLrtyfTRbZDrlWFBqpY0EbiEBvoztns8X24+SpGziU+k1WcqXPx3293w07sq/jArccLHv7QNpeP/AkOnn9s5R9wNzkLbJ91bVr8Ex/fAxX+D6+ZC0Sl4b1rl4yu++ScEN7MfiOX1vNLOwHnke8/GXN7uxbY0027k2dv8g2zJZdtndsBZecbY32nGDrjmdTvl+ci74aYv4P/2wNWvQZeJsHepPdawX9YstpBmcO2b9rq8NaXiGFSDpImgjEv7JHA8t5CVe3zgbmbobXDBTNjwNiz8/ZnbSkrg01/Dpnkw5ne218i5iulo2xDWvvpjlUR9yj5q2wa6TITO46BFd7hilp2a4bN7zk6G6Tvt0qFDpkNQ+NnH634pOPzrf3DZniW2OquimMBWDzkL7B19ecufhG2fwrg/Q4fRZ24LaQ69rrTX5P5dcN92aNa65vF1GA0/ecf2HHvrCu93V07bBvuWezeGRkATQRkXdI0jIsifz75v4tVDpUbPhMG32oVyvnnaPmcMfHG/rV644EHbb76ujLzHDnxa+2rdHdNdix6xjd4T/vrjc90vte/x+3dh9ctn7r/yGTvl8tDbKj5eaLT90NvyUf1VD2Wn2obrqur/E/rY+avKjynYOd+OLu89FYb/qurzOBwQHFX7ODtdBNe+A6lb7Oyo7nRMSNtmk1ddXsviAnhnKrxxCXw4XUsoVdBEUEaQvx/je7bkqy1HKSj2gUnaRGwVUa+rbde/dW/AVw9B8qu2Kmf0Q3V7voS+dmDbty/aKRvqS0qy/bAf/quz13y4YCZ0nQzzf2P74YNtN/j+PRhwo50gsDI9r4STB2wPnHPlzgfg3iX2e1WJAKD/jbbKqnQBo4xdtq2nZW+49Jn66fLaZTxMfQOObIB3rrZtExVJ3QLzfgYvDrdtE/uW1l0M696wEzv2vsYmmReHw46v6u74TYgmgnIu7ZtAdn4xy3dmeDuU+uFw2MbjThfZ6qDVL9mlLi/6o2c+MEbdC7npnp0kraySEtvDJ7xlxaUbh8NWh8R0sh9IJ36wicqU/DjlcmW6TQa/wHOrHtq9GGaNhNkXVD9X0O7FEBpr7/ir0vtqG9eGd+zd+L+vtw37P3nH9vmvL90mw9VzbCJ+91o72LDU0c22sf6l8+z7GnWv7Z319eN1UyoozLVdWduOtN2Zf7EYQmNg7rXw0QzIO1H9MXyIJoJyRnSKpXloAJ829d5DZfkH2m6lXSfBeb+GiY957q6x7QjbRXHls9WveXt0849z7tfW93NtO8BFf7RdHCsSHGlX3Spx2g/Nda9Dr6vO7KdfkZBmtoSz5aOar9R1dJOtQ3/7Sjh1wt7Bz69iIFhJiS0RdLyw6lG+YKutul1iZ5z9cLrtLnrNG3am0PrWY4odSHhgFcz9CRxca3stzRphRy6f/39w90Y7/mTUvXa/uigVrH4ZctPscUWgVT+Y/rVdAGrjPFs68NT02WtfhQUPe799pAY0EZQT4OdgYq8EFm1NJa/QB6qHSgWG2Z404//s2aoDERh5r61SqexOusRpe+zMvsDWMf/3jtrNrJmfZddiSBpc/ejY2E5w9au2qqIwx/amcUevK+10DyluLgmaeQg++iXMGmWrlCb8FX693g5YW/cabP1vxa9L3WRLUh3Hunee/tPsXe/OL+05qlu7wpN6XQWXz7KNtq9eZL9fMNMmgDG/tYkL7NiIuigV5J204z+6TIQ2Q3983j8QxjwMty62vcHenWqnP6nLtTp2fAWf32vHYTw/yK7q1gim6NZEUIFL+ySQW+hkyQ4v9nlvyrpMtGvefvPPs++kMw/Bm1PsB3jXSbat4ru37Yjm1BqO5F32d3tXePHfqr+LBtubaMoLMPo37g8U6zIR/IKqX8g9P9M2WD83wNZXn3cn3LXBtlv4B8GFD9uZRD+50ybJ8vb8z37veKF7cXUYDa36w6BbKm/wrk99r7WlzrF/sAngwodsT6Wy/IN+LBWcy7TWK5+113vMwxVvb9UfbltqS7/r37RJoy4c22NLYC37wM0LoFlbO6bi9ck1/9utb8aYRvU1cOBA42nFzhIz8M8LzYy3kj1+Lp+1Ya4xf4g0ZvuXPz635WNjHmtjzF8SjFn3pjElJfb53YuN+XsnY/7cwpjk1358virpO415JMaYj2/3SPhnmHu9MU90McZZfPY2Z7Exa+cY87f29v2+/wtjTvxQ8XGO7THm0URj/jXemOKiM7e9NtmYF8+rWVzuXKeGpijfmCe72WtQm/izU435S0tj/nNT9fuWlBgz72fGPBJtTMo5/q8X5Nrfz2NtjDm+zz7ndNq/18fb2nPM/60x+dmVH6O40JjMw8ZkHTEmJ8OYUyeMKcgxpqigTn6XQLKp5HNVp5iogJ9DuKRPAnPXHCCnoJjwIL1Mda7XVbY74zdP2cFRX820XR5bDYCr/nVm756OY2DGN/bu6tO77ICnS5+peJ3nkhI7U+gXD0BAiL0D9fh7udIOzjuw6syBXvu/gS9n2mqdNufBxL/au9HKRHeAS/4JH/7ClmZKJ48rzIUD39Z8gJc3JsQ7V6Wlgi/ut6UCd0tApZb/w3YbHe3GxHsi9nofXGt7Vd22vPLxGVUxxv5dpm6Bae/bqVzAlkIH/hy6XWp75a18zk5Lct6v7ejwrCO2WjHrsP3KSQWqqEYSP1tlOfb3le9TS/oJV4lL+iTw+sr9LNqayuX9azhTp6qeXwCM+LX9h39+sJ29dNR9tstqRVNXRMTDtA9hxT9tAjm83v4TFxfapTjTttvv6TvtPxnAhMfsfEie1mUiBITaf/J2I23Po4W/s/X9Ua3tiN2eV7j3wdznGlsNtOwJO41Fu5E2oZQUVd9ttKkY8FNY/pRtK+gw2v2EdvIAJM+B/jfYNh93hDSHK2fbsQZfPgiXv1DzeNe8YgdfXviw7X1XXliMXWe8/43w+X3wlWt59uAo2yYS2cpWRUYm/vj3WuK0v3Nnket7sf3e9ryax+cGTQSVGNCmOa2igvls42FNBJ7Sf5pdLEccdgK4iqZNKMvhsMmi7Qh4/xbb66ZURCto0Q0G3QRx3aBlL0isp+mQA8OgywT7wR8aDSuete9p9G9sW0BNu2xOegIOroYPboVfrrCJwT/ELhfpC2pbKlj6N/v9ggdrdr52I+zf1bInoNNYW8Jz14FvYf5D0OVie4yqtBlq2yZOHrDrN9Sm9OEhYhpBi3ZZgwYNMsnJ9bOq5aOfb+X1lftJ/u04okLdnGBN1UzuMTsDamBYzV6Xd8L2P49qDXFdazY5mids/QTm3Wh/7n2N7a4alVT74x3eAP+6yM4ImrHTdmWdVsG0EU1VcQE829/+fm/+qvpSQcYueGGInTZ74mM1P5+zCOZMtMf55Qr3ptfIToWXz7dVkNO/9v7fYDVEZJ2pZCVI7TVUhUv7tqLIaZi/5ai3Q2m6wmJqngTAFul7X23vshrCP2CXifZO9Ob5to3jXJIA2H7v4x6xM78e2+V+t9GmorRUcPBb93oQLXnUVs9Vd1deGb8AO97BOG3Pn+q6lDqL4D8/t72Trn27YfwNngNNBFXonRhF25hQ3xpcpmrHP9A27la3vkFNDP0ldBpnf/aV9oGy+t9o682rG1dweIPtvjvs9qqnBKlOdAe7XsaBlbYTQ0WMsaWGT++y+132rK2GbOS0jaAKIrb30KylezmWU0BMuBsrNSlVVxwOO8jth1W2/cPXlJYKPr/v7LYCYyDzIBxcA6tesAPEzqtmShB39LkWdi2EJY9BhwshaZAdoLZvqa2K3LPEzl8EdgqSPlOrPl4joYmgGpf2bcULS/bw5eajTBtWzZQDStW14CjoOtHbUXhP/xtdPYges1WIB1fbD/+Da+x62WCrhCY9eW4zppYSgUuessef9zO7AFNKsq0yCoyADhfYLpwdx0B0+3M/XwOhiaAaXeMj6BofwZur9nPdkDb4ORph32ylGquypYJXXdVkzdraKTOShkDrIRDfC/zq8KMsOMq2F7x9le3OOepe20aTNMj9VfkaGU0E1RAR7hzbiTve/Y7/bjjElQPOsRFQKVUzA35m+9FHJdoP/4h4z5+zzTD4TRXLmDYx2p1g8IcAABvQSURBVFjshkm9EuiVGMlTC3dSWNzEl7FUqqHxC4BhM+xCQvWRBHyQJgI3OBzCAxO6kXIij7lrKpgQTCmlGjFNBG46v3MswzpE89z/dpNbUM08+kop1Yh4LBGIyBwRSRORzZVs7yYiq0SkQETqcGFczxAR/m9iNzJyCnhtxT5vh6OUUnXGkyWC14Gq+r0dB34NPOnBGOrUgDbNGdcjnpeX7uVEbqG3w1FKqTrhsURgjFmG/bCvbHuaMWYtUOSpGDzhgQldySksZtbSPd4ORSml6kSjaCMQkekikiwiyenp6V6NpUt8BFf0T+T1lfs5mpnv1ViUUqouNIpEYIyZbYwZZIwZFBcX5+1wuOeiLpQYwzOLd3k7FKWUOmeNIhE0NK2jQ7lhaFvmJR9kX0aut8NRSqlzoomgln51YSeC/B38Y8EOb4eilFLnxJPdR+cCq4CuIpIiIreIyAwRmeHa3lJEUoB7gYdd+1SwCG3DFBcRxC0j2/PZxiNsPpTp7XCUUqrWPDbXkDHmumq2HwUa9cQ9t57fgbe+/YEn5u/gjZuHeDscpZSqFa0aOgeRwQHcProjS3ems3rvMW+Ho5RStaKJ4Bz9dHg74iODeHLBDhrb+s9KKQWaCM5ZcIAfd47pzNr9J1i607tjHJRSqjY0EdSBqYNa0yY6lCfm76CkREsFSqnGRRNBHQj0d3DPuM5sOZzFV1uOejscpZSqEU0EdeSyvol0bhHOPxbsoNipi9copRoPTQR1xM8h3De+K3vSc/noO99Z4k4p1fhpIqhDE3rG0ycpiqcX7aKg2OntcJRSyi2aCOqQiPDAhK4cOpnHe2sPejscpZRyiyaCOjayUyxD20fz7OLdnCrUJS2VUg2fJoI6VloqyMgp4I2VP3g7HKWUqpYmAg8Y1C6aMd1aMGvpHjLzGtUCbEopH+RWIhCRu0QkUqxXRWS9iIz3dHCN2X3ju5CZV8S/lu/1dihKKVUld0sENxtjsoDxQHPgRuBxj0XVBPRsFcXkPgm8+s0+Uk6c8nY4SilVKXcTgbi+TwLeMsZsKfOcqsSDE7rhJ8Jtb60jr1C7kyqlGiZ3E8E6EVmATQTzRSQC0OGz1WgTE8oz1/Vj65EsHvxgo85OqpRqkNxNBLcAM4HBxphTQABwk8eiakLGdIvn/vFd+eT7w7yi7QVKqQbI3UQwHNhhjDkpItOAhwFdn9FNt4/uyKTeLXn8y+06VbVSqsFxNxG8BJwSkb7AfcAe4E2PRdXEiAhPXN2XLvER3PnuevZn5Ho7JKWUOs3dRFBsbAX3FOB5Y8wLQITnwmp6woL8mX3jIBwOYfpbyeQU6KhjpVTD4G4iyBaRh7DdRj8XEQe2nUDVQJuYUJ6/bgC703K4b94GXcRGKdUguJsIrgUKsOMJjgJJwBMei6oJG9k5lt9M6s78Lak8v2S3t8NRSin3EoHrw/8dIEpELgHyjTHaRlBLt4xszxX9E3lq4U4+XJ/i7XCUUj7O3SkmpgJrgGuAqcBqEbnak4E1ZSLCY1f2ZniHGO6d9z2vLNNupUop7/F3c7/fYscQpAGISBywCHjfU4E1dcEBfrx+82DueW8Dj36xjbTsfB66uDsOhw7YVkrVL3cTgaM0CbgcQ2cuPWdB/n48d90AYsO38MryfaRnF/D3q/sS6K+XVilVf9xNBF+JyHxgruvxtcAXngnJt/g5hEcu60mLiCCeXLCTY7mFzJo2kLAgd381Sil1btxtLH4AmA30cX3NNsY8WNVrRGSOiKSJyOZKtouIPCsiu0Vko4gMqGnwTYWIcMeYzvztqt6s2J3Bda98S0ZOgbfDUkr5CLfrIIwxHxhj7nV9feTGS14HJlax/WKgs+trOnb0sk+7dnAbXr5xEDuOZnP1Sys5eFynr1ZKeV6ViUBEskUkq4KvbBHJquq1xphlwPEqdpkCvGmsb4FmIpJQ87fQtIzrEc+7tw7lxKkifvbaGh2BrJTyuCoTgTEmwhgTWcFXhDEm8hzPnQgcLPM4xfXcWURkuogki0hyenrTn7RtYNtoZk0byP6MXJ2+WinlcY2ie4oxZrYxZpAxZlBcXJy3w6kXwzvGcP+Erny+8Qivr9zv7XCUUk2YNxPBIaB1mcdJrueUy4zzO3JR9xY8+vk21v1wwtvhKKWaKG8mgk+An7p6Dw0DMo0xR7wYT4PjcAj/uKYfCc2CuePd9RzTnkRKKQ/wWCIQkbnAKqCriKSIyC0iMkNEZrh2+QLYC+wGXgFu91QsjVlUaAAv3TCQY7mF3P3eBpw6Y6lSqo55bNSSMea6arYb4FeeOn9T0isxij9d1pOZH27imcW7uHdcF2+HpJRqQhpFY7GCawe35uqBSTz3v118vSOt+hcopZSbNBE0EiLCn6f0omt8BHe/t4GUEzrYTClVNzQRNCIhgX68NG0gTqfhptfWsuHgSW+HpJRqAjQRNDLtY8N4adpAMvOKuOLFFcz8YCPHcwu9HZZSqhHTRNAIjewcy+L7LuAXI9vz/roULnzya9769gftUaSUqhVNBI1URHAAv53cgy/vGkXPVpH87uPNXPb8NzrwTClVY5oIGrnO8RG884uhPH99f47lFHLVSyu5b973HD6Z5+3QlFKNhK5+0gSICJf0acWFXVvw/JLdvLp8H59+f5gbhrXh9tGdiIsI8naISqkGTBrbzJaDBg0yycnJ3g6jQTt0Mo9nF+3i/fUpBPo5uGlEO247vyNRoQHeDk0p5SUiss4YM6jCbZoImq59Gbn8c+FOPt14mPAgf24d1YGbR7YnXJfBVMrnVJUItI2gCWsfG8az1/Xny7tGMbxDDE8t3Mn5f1/CV5uPejs0pVQDoonAB3RrGcnsnw7iv78aQevmIcx4ex0vLNmtC94opQBNBD6lb+tmvHfbcKb0a8UT83dwz3sbyC9yejsspZSXaWWxjwkO8OPpa/vRuUU4Ty7YyQ/HT/HyjQNpERHs7dCUUl6iJQIfJCLcMaYzL90wgO1Hsrn8+RVsPZzl7bCUUl6iicCHXdw7gf/MGE6JgatnrWT+Fm1EVsoXaSLwcb0So/jkjhF0jo9gxtvr+OfCnRQ7S7wdllKqHmkiULSIDOa96cO4on8izyzexVWzVrEnPcfbYSml6okmAgXYRuSnpvbjhesH8MOxXCY/u5w3V+3XLqZK+QBNBOoMk/skMP/u8xnaPobf/3cLP52zhqOZ+d4OSynlQZoI1FniI4N5/abB/OXyXiTvP8GEp5fxyfeHvR2WUspDNBGoCokI04a15Yu7RtE+Noxfz/2OX7yxlm92ZVCiC+Ao1aTogDJVpfaxYbw/YzgvL9vL7GV7WbQtjaTmIVw7qDVXD0oiISrE2yEqpc6Rzj6q3JZf5GT+lqPMSz7Iit3HcAiM7tqCqYNaM7Z7CwL8tICpVEOl01CrOnfg2CnmJR/kP+sOkppVQGx4IBN6tmRS7wSGto/GX5OCUg2KJgLlMcXOEpbuTOeD9Sks2Z5OXpGT5qEBjO/Rkot7t+S8jrEE+mtSUMrbNBGoepFX6GTpznS+3HyExdvSyCkoJjLYn4t6xDO5dwIjO8cS5O/n7TCV8kleSwQiMhF4BvAD/mWMebzc9rbAHCAOOA5MM8akVHVMTQSNQ36RkxW7M/hy81EWbDlKVn4xEcH+jO/Rkkv6JDCik5YUlKpPXkkEIuIH7ATGASnAWuA6Y8zWMvv8B/jMGPOGiIwBbjLG3FjVcTURND6FxSWs2JPB5xuPMH/LUbLzbUlhQs+WTOqTwAitPlLK47yVCIYDfzTGTHA9fgjAGPNYmX22ABONMQdFRIBMY0xkVcfVRNC4FRaX8M3udD7beISFW1LJdlUfje0ez4Se8ZzfJY7QQO3VrFRdqyoRePI/LhE4WOZxCjC03D7fA1diq4+uACJEJMYYc6zsTiIyHZgO0KZNG48FrDwv0N/BmG7xjOkWT0Gxk+U7M/hqy1EWbUvlo+8OERzg4PzOcUzs1ZKx3eKJCg3wdshKNXnevvW6H3heRH4OLAMOAWetnWiMmQ3MBlsiqM8AlecE+ftxUY94LuoRT7GzhDX7jjN/y1Hmb0llwdZU/B3CmG4tePDibnSMC/d2uEo1WV6tGiq3fziw3RiTVNVxtWqo6SspMXyfcpKvNh/l3dUHyCty8vPz2nHn2M5EhWgJQanaqKpqyJMtdGuBziLSXkQCgZ8An5QLLFZESmN4CNuDSPk4h0Po36Y5D03qzv/uH83VA5N4dcU+xjz5NXPXHMCpcx0pVac8lgiMMcXAHcB8YBswzxizRUT+JCKXuXYbDewQkZ1APPCop+JRjVNcRBCPX9WHT+8YSYe4MB76cBOXPvcNq/ceq/7FSim36IAy1WgYY/h80xH++vk2DmfmM75HPON7tmRIu2haR4dgO54ppSrirV5DStUpEeGSPq0Y2y2el5ft4fWV+1mwNRWAlpHBDG4fzZD20QxpF03nFuE4HJoYlHKHlghUo1VSYtiVlsOafcdYve84a/cfJzWrAIBmoQH0Toyid2IUfZKi6JUYRWIzLTUo36VzDSmfYIzhwPFTrHElhU2HstiZmn26cbl5aAC9XMnhoh7xDGjT3MsRK1V/NBEon5Vf5GTbkSw2H8pk06FMNh3KYldqNsUlhoFtm3PrqA6M6xGPn1YjqSZO2wiUzwoO8KN/m+b0L3P3n1tQzH+SD/Lqin3MeHsdbWNCuWVke64emKTTWyifpCUC5bOcJYb5W44ye9leNhw8SbPQAKYNbcuNw9sSHxns7fCUqlNaNaRUFYwxrPvhBK8s33u6F1K/1s0Y1yOecd3j6dQiXBuZVaOniUApN+3PyOXT7w+zcFsqG1MyAWgXE8pF3eMZ1yOegW2b6zKcqlHSRKBULRzNzGfRtlQWbUtl5e5jFDpLiA0PZMYFHZk2rC3BAbrammo8NBEodY5yCopZvjOdd1Yf4JvdGbSKCubui7pw5YBELSGoRkETgVJ1aOXuDP42fwffHzxJx7gw7hvflYt7tdR2BNWgafdRperQeZ1i+bhjDAu2pvLk/B3c/s56eidGce/4LnSIDaPIaSguKaHYaShyluAsMRSXGLq3jNSFdlSDpCUCpc6Bs8Tw8XeHeGrhTg6dzKty34ggf249vwM3j2xPeJDeg6n6pVVDSnlYQbGTxdvSyCt04u8nBPg58HfY734OwWkMc1cfYMHWVKLDArl9tDY4q/qliUCpBmLDwZP8Y8EOlu/KoGVkMHeO7cTUQa0J0AZn5WGaCJRqYFbuyeDJ+TtYf+AkbWNCuWZgEmFB/gT5+xHk7yAowHH65+iwQHq2itTGaHVONBEo1QAZY1iyI40n5+9k65GsKvftEBvGtYNbc9XAJGLDg+opQtWUaCJQqgEzxnCq0ElhcQkFxSUUFJ/58570XOatPUjyDyfwdwjjesTzkyFtGNUpVhffUW7T7qNKNWAiQliQP2GV3OgPbBvN1EGt2ZWazXtrD/LB+hS+3HyUxGYhXDMoiUFto+nUIpz4yCCtPlK1oiUCpRqZgmInC7ak8t7ag3yzO+P08+FB/nSMC6NjXDgdW4TTMS6coe2jaR4W6MVoVUOhVUNKNVHp2QXsSstmT1oOe9Jz2Z2Ww570HI5k5gMQ6O9gcu8EbhjahoFtm2uJwYdp1ZBSTVRcRBBxEUGc1zH2jOdzCorZcTSb/244xEfrD/HRd4foGh/BDcPacHn/RCKDdYSz+pGWCJRq4k4VFvPp94d5Z/UBNqZkEhLgx2V9W3FB1zhK25pLPwZKPw2KSwx5hcXkFjg5VVhMbqGTUwX2e0mJ4cbhbc9Y9U01fFo1pJQCYGPKSd5dfYD/bjhMXpHT7dcF+jkIDfIjLNCfnIJisvOLuH10J349tjOB/joYrjHQRKCUOkN2fhEHjp9CEEqbDU5/R3AIhAb5Ex7oT0ig3xkf9ln5Rfz50638Z10K3RMieWpqX7onRHrhXaia0ESglKpzC7em8tCHm8jMK+SecV2YPqqDrs3QgFWVCDz6WxORiSKyQ0R2i8jMCra3EZElIvKdiGwUkUmejEcpVXfG9YhnwT3nM75HS/7+1Q6ueXkVe9NzvB2WqgWPJQIR8QNeAC4GegDXiUiPcrs9DMwzxvQHfgK86Kl4lFJ1LzoskOev78+z1/Vnb3ouk55dzsffHfJ2WKqGPFkiGALsNsbsNcYUAv8GppTbxwCllYtRwGEPxqOU8gAR4bK+rVhwz/n0TWrGvfM28NlG/VduTDyZCBKBg2Uep7ieK+uPwDQRSQG+AO70YDxKKQ+KjwzmtZsGM7Btc+7+9wYWb0v1dkjKTd5u2bkOeN0YkwRMAt4SkbNiEpHpIpIsIsnp6en1HqRSyj2hgf68+vPB9GgVyS/fWc83uzKqf5HyOk8mgkNA6zKPk1zPlXULMA/AGLMKCAZiy+2DMWa2MWaQMWZQXFych8JVStWFyOAA3rhpCO1jwrj1zWTW7j/u7ZBUNTyZCNYCnUWkvYgEYhuDPym3zwFgLICIdMcmAr3lV6qRax4WyNu/GEpCVDA3v7aWjSknvR2SqoLHEoExphi4A5gPbMP2DtoiIn8Skctcu90H3Coi3wNzgZ+bxjawQSlVobiIIN65dShRoQH8dM4ath+tevEd5T06oEwp5VEHjp3impdX4iyBWdMGEB0WiEPsiOayI5uD/B3EhAfhp4vteISOLFZKedXutByufXkVx3ILq9zPzyHERwTRMiqYhKgQEqKCaRkVTFLzEEZ2jiM8SCdMri2dhlop5VWdWoTz6Z0jWbv/OMaAwdjvBkqMwQAFRU5Sswo4nJnH0cx8th3JYvH2VPKLSgAIC/RjSv9Erh/Shl6JUd59Q02MJgKlVL1o1SyEKf3KDyWqmjGGzLwidqbmMC/5IB+uT+Hd1QfomxTF9UPbcGnfVoQG6sfYudKqIaVUo5F5qogPv7PJYFdaDhFB/lzeP5HJfRLo36YZQf5+3g6xwdI2AqVUk2KMIfmHE7y7+gCfbzpCYXEJQf4OBrRpzrAOMQzvGEPf1lGVJgZnieHkqUKKSwwtIoJ8YglPTQRKqSYrM6+INfuO8+3eY6zac4xtR7MwBoIDHAxs25zEZiGcOFXEidxCjp8q5HhuIZl5RadXZYsKCaBHQiTdEyLp0SqSHgmRdGoR7taCO5mnitidns2u1Bx2peWw2/XVIjKImRO7MbRDjIffvfs0ESilfMbJU4Ws2XecVXuP8e3e4xzPLaB5aCDRYYE0DwskOrT0ewAiwvaj2Ww7ksX2o1mnG6YD/IT2sWGEBLhKFK4Sg7h+NAYOncwjPbvg9HmDAxx0jAunY1w4yfuPczgzn4t7teShi7vTJia0vi/DWTQRKKVUNZwlhv3Hctl6OIttR7LYlZZDkbPkjPWcy35etowMplOLcDrHh9O5RQSJzUJwuMZA5BU6eWX5Xl76eg/OEsNNI9txx4WdiAgOqFE8h07ksTcjh30ZuexNz2VEpxgm9kqo1fvTRKCUUl6QmpXP37/awQfrU4gND+S+8V2ZOqg1DoHcQidpWfmkZxeQVvqVlc++jFz2ZeTyw7FTFDpLTh8rIsifGaM78qsLO9UqFk0ESinlRRtTTvLnz7aydv8JosMCySt0klfkPGu/QD8HbWJCaR8bRoe4MDrEhtE+NpwOcWHEhAWeU6O2DihTSikv6pPUjHm3DefLzUdZtDWV5mGBtIgIIi4iiBYRwa7vQUSFBJyuXqpPmgiUUqoeiAiTeicwqXft6vg9ydsL0yillPIyTQRKKeXjNBEopZSP00SglFI+ThOBUkr5OE0ESinl4zQRKKWUj9NEoJRSPq7RTTEhIunAD7V8eSyQUYfhNEV6jaqm16d6eo2q5q3r09YYE1fRhkaXCM6FiCRXNteGsvQaVU2vT/X0GlWtIV4frRpSSikfp4lAKaV8nK8lgtneDqAR0GtUNb0+1dNrVLUGd318qo1AKaXU2XytRKCUUqocTQRKKeXjfCYRiMhEEdkhIrtFZKa342kIRGSOiKSJyOYyz0WLyEIR2eX63tybMXqTiLQWkSUislVEtojIXa7n9RoBIhIsImtE5HvX9XnE9Xx7EVnt+l97T0QCvR2rN4mIn4h8JyKfuR43uOvjE4lARPyAF4CLgR7AdSLSw7tRNQivAxPLPTcTWGyM6Qwsdj32VcXAfcaYHsAw4Feuvxu9RlYBMMYY0xfoB0wUkWHA34B/GmM6ASeAW7wYY0NwF7CtzOMGd318IhEAQ4Ddxpi9xphC4N/AFC/H5HXGmGXA8XJPTwHecP38BnB5vQbVgBhjjhhj1rt+zsb+Myei1wgAY+W4Hga4vgwwBnjf9bzPXh8AEUkCJgP/cj0WGuD18ZVEkAgcLPM4xfWcOlu8MeaI6+ejQLw3g2koRKQd0B9YjV6j01zVHhuANGAhsAc4aYwpdu3i6/9rTwP/B5S4HsfQAK+PryQCVQvG9i32+f7FIhIOfADcbYzJKrvN16+RMcZpjOkHJGFL3t28HFKDISKXAGnGmHXejqU6/t4OoJ4cAlqXeZzkek6dLVVEEowxR0QkAXun57NEJACbBN4xxnzoelqvUTnGmJMisgQYDjQTEX/XXa8v/6+NAC4TkUlAMBAJPEMDvD6+UiJYC3R2tdYHAj8BPvFyTA3VJ8DPXD//DPivF2PxKld97qvANmPMU2U26TUCRCRORJq5fg4BxmHbUZYAV7t289nrY4x5yBiTZIxph/3M+Z8x5gYa4PXxmZHFrqz8NOAHzDHGPOrlkLxOROYCo7HT4qYCfwA+BuYBbbDTfU81xpRvUPYJIjISWA5s4sc63t9g2wl8/hqJSB9sY6cf9qZynjHmTyLSAdshIxr4DphmjCnwXqTeJyKjgfuNMZc0xOvjM4lAKaVUxXylakgppVQlNBEopZSP00SglFI+ThOBUkr5OE0ESinl4zQRKFWPRGR06SyUSjUUmgiUUsrHaSJQqgIiMs011/4GEXnZNblajoj80zX3/mIRiXPt209EvhWRjSLyUen6BCLSSUQWuebrXy8iHV2HDxeR90Vku4i84xrBrJTXaCJQqhwR6Q5cC4xwTajmBG4AwoBkY0xPYCl2JDbAm8CDxpg+2FHIpc+/A7zgmq//PKB0xtL+wN3YtTE6YOekUcprfGXSOaVqYiwwEFjrulkPwU4sVwK859rnbeBDEYkCmhljlrqefwP4j4hEAInGmI8AjDH5AK7jrTHGpLgebwDaAd94/m0pVTFNBEqdTYA3jDEPnfGkyO/K7Vfb+VnKzivjRP8PlZdp1ZBSZ1sMXC0iLeD0GsVtsf8vpbNGXg98Y4zJBE6IyCjX8zcCS10rmqWIyOWuYwSJSGi9vgul3KR3IkqVY4zZKiIPAwtExAEUAb8CcoEhrm1p2HYEsFMJz3J90O8FbnI9fyPwsoj8yXWMa+rxbSjlNp19VCk3iUiOMSbc23EoVde0akgppXyclgiUUsrHaYlAKaV8nCYCpZTycZoIlFLKx2kiUEopH6eJQCmlfNz/A+578WCRW4UWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model_info.history.keys())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_info.history['loss'])\n",
    "plt.plot(model_info.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('rong_test_2_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_info = model.fit_generator(\n",
    "#             train_generator,\n",
    "#             steps_per_epoch=nb_train_samples // batch_size,\n",
    "#             epochs=epochs,\n",
    "#             callbacks = callbacks,\n",
    "#             validation_data=validation_generator,\n",
    "#             validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.plot(model_info.history['loss'])\n",
    "# plt.plot(model_info.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('rong_test_2.h5')\n",
    "# # history = model.fit_generator(\n",
    "# #     train_generator,\n",
    "# #     steps_per_epoch = nb_train_samples // batch_size,\n",
    "# #     epochs = epochs,\n",
    "# #     callbacks = callbacks,\n",
    "# #     validation_data = validation_generator,\n",
    "# #     validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import sklearn\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import numpy as np\n",
    "# # Found 28709 images belonging to 7 classes.\n",
    "# # Found 3589 images belonging to 7 classes.\n",
    "\n",
    "\n",
    "# # nb_train_samples = 28273\n",
    "# # nb_validation_samples = 3534\n",
    "# nb_train_samples = 140\n",
    "# nb_validation_samples = 70\n",
    "\n",
    "# # We need to recreate our validation generator with shuffle = false\n",
    "# validation_generator = val_datagen.flow_from_directory(\n",
    "#         validation_data_dir,\n",
    "#         color_mode = 'grayscale',\n",
    "#         target_size=(img_rows, img_cols),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical',\n",
    "#         shuffle=False)\n",
    "\n",
    "# class_labels = validation_generator.class_indices\n",
    "# class_labels = {v: k for k, v in class_labels.items()}\n",
    "# classes = list(class_labels.values())\n",
    "\n",
    "# #Confution Matrix and Classification Report\n",
    "# Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "# y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# print('Confusion Matrix')\n",
    "# print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "# print('Classification Report')\n",
    "# target_names = list(class_labels.values())\n",
    "# print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "# plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "# tick_marks = np.arange(len(classes))\n",
    "# _ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "# _ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# classifier = load_model('./emotion_detector_models/model_v3_71.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_generator = val_datagen.flow_from_directory(\n",
    "#         validation_data_dir,\n",
    "#         color_mode = 'grayscale',\n",
    "#         target_size=(img_rows, img_cols),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical',\n",
    "#         shuffle=False)\n",
    "\n",
    "# class_labels = validation_generator.class_indices\n",
    "# class_labels = {v: k for k, v in class_labels.items()}\n",
    "# classes = list(class_labels.values())\n",
    "# print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# from keras.optimizers import RMSprop, SGD, Adam\n",
    "# from keras.preprocessing import image\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "# import re\n",
    "\n",
    "# def draw_test(name, pred, im, true_label):\n",
    "#     BLACK = [0,0,0]\n",
    "#     expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "#     cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "#     cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "#     cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "# def getRandomImage(path, img_width, img_height):\n",
    "#     \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "#     folders = path\n",
    "#     random_directory = np.random.randint(0,len(folders))\n",
    "#     path_class = folders[random_directory]\n",
    "#     file_path = path\n",
    "#     file_names = [f for f in listdir(file_path)]\n",
    "#     random_file_index = np.random.randint(0,len(file_names))\n",
    "#     image_name = file_names[random_file_index]\n",
    "#     final_path = file_path + \"/\" + image_name\n",
    "#     return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# # dimensions of our images\n",
    "# img_width, img_height = 48, 48\n",
    "\n",
    "# # We use a very small learning rate \n",
    "# model.compile(loss = 'categorical_crossentropy',\n",
    "#               optimizer = RMSprop(lr = 0.001),\n",
    "#               metrics = ['accuracy'])\n",
    "\n",
    "# files = []\n",
    "# predictions = []\n",
    "# true_labels = []\n",
    "\n",
    "# # predicting images\n",
    "# for i in range(0, 10):\n",
    "#     path = '../non_load/fer2013/train' \n",
    "#     img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "#     files.append(final_path)\n",
    "#     true_labels.append(true_label)\n",
    "#     x = image.img_to_array(img)\n",
    "#     x = x * 1./255\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     images = np.vstack([x])\n",
    "#     classes = model.predict_classes(images, batch_size = 10)\n",
    "#     predictions.append(classes)\n",
    "    \n",
    "# for i in range(0, len(files)):\n",
    "#     image = cv2.imread((files[i]))\n",
    "#     image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "#     draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "#     cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('../datasets/classifiers/haarcascade_frontalface_default.xml')\n",
    "classifier =load_model('rong_test_2_9.hdf5')\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
