{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# code credits: Durgesh Thakur\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../test/train'\n",
    "validation_data_dir = '../test/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "img_rows,img_cols = 48,48\n",
    "batch_size = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=30,\n",
    "                                   shear_range=0.3,\n",
    "                                   zoom_range=0.3,\n",
    "                                   width_shift_range=0.4,\n",
    "                                   height_shift_range=0.4,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16944 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    color_mode='grayscale',\n",
    "                                                    target_size=(img_rows,img_cols),\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2099 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                              color_mode='grayscale',\n",
    "                                                              target_size=(img_rows,img_cols),\n",
    "                                                              batch_size=batch_size,\n",
    "                                                              class_mode='categorical',\n",
    "                                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,327,907\n",
      "Trainable params: 1,325,731\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# layers\n",
    "model = Sequential()\n",
    "\n",
    "# Block-1\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-2 \n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-3\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-4 \n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-5\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block-6\n",
    "\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block-7\n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks\n",
    "checkpoint = ModelCheckpoint('rong_test1_fer2.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "# earlystop = EarlyStopping(monitor='val_loss',\n",
    "#                           min_delta=0,\n",
    "#                           patience=9,\n",
    "#                           verbose=1,\n",
    "#                           restore_best_weights=True\n",
    "#                           )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "# callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "callbacks = [checkpoint,reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-c7c7309de9d4>:11: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.6268 - accuracy: 0.3349\n",
      "Epoch 00001: val_loss improved from inf to 1.47184, saving model to rong_test1_fer2.h5\n",
      "141/141 [==============================] - 28s 199ms/step - loss: 1.6268 - accuracy: 0.3349 - val_loss: 1.4718 - val_accuracy: 0.3907 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.3240 - accuracy: 0.3650\n",
      "Epoch 00002: val_loss improved from 1.47184 to 1.10550, saving model to rong_test1_fer2.h5\n",
      "141/141 [==============================] - 28s 195ms/step - loss: 1.3240 - accuracy: 0.3650 - val_loss: 1.1055 - val_accuracy: 0.4201 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.2144 - accuracy: 0.3785\n",
      "Epoch 00003: val_loss improved from 1.10550 to 1.08275, saving model to rong_test1_fer2.h5\n",
      "141/141 [==============================] - 28s 198ms/step - loss: 1.2144 - accuracy: 0.3785 - val_loss: 1.0827 - val_accuracy: 0.4275 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.1535 - accuracy: 0.3830\n",
      "Epoch 00004: val_loss improved from 1.08275 to 1.08105, saving model to rong_test1_fer2.h5\n",
      "141/141 [==============================] - 27s 190ms/step - loss: 1.1535 - accuracy: 0.3830 - val_loss: 1.0810 - val_accuracy: 0.4255 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.1167 - accuracy: 0.3812\n",
      "Epoch 00005: val_loss improved from 1.08105 to 1.07619, saving model to rong_test1_fer2.h5\n",
      "141/141 [==============================] - 27s 190ms/step - loss: 1.1167 - accuracy: 0.3812 - val_loss: 1.0762 - val_accuracy: 0.4235 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0983 - accuracy: 0.4064\n",
      "Epoch 00006: val_loss improved from 1.07619 to 1.07609, saving model to rong_test1_fer2.h5\n",
      "141/141 [==============================] - 28s 198ms/step - loss: 1.0983 - accuracy: 0.4064 - val_loss: 1.0761 - val_accuracy: 0.4206 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0953 - accuracy: 0.4090\n",
      "Epoch 00007: val_loss did not improve from 1.07609\n",
      "141/141 [==============================] - 27s 194ms/step - loss: 1.0953 - accuracy: 0.4090 - val_loss: 1.0780 - val_accuracy: 0.4270 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0873 - accuracy: 0.4037\n",
      "Epoch 00008: val_loss did not improve from 1.07609\n",
      "141/141 [==============================] - 28s 198ms/step - loss: 1.0873 - accuracy: 0.4037 - val_loss: 1.0810 - val_accuracy: 0.4206 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0836 - accuracy: 0.4161\n",
      "Epoch 00009: val_loss did not improve from 1.07609\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "141/141 [==============================] - 37s 265ms/step - loss: 1.0836 - accuracy: 0.4161 - val_loss: 1.0771 - val_accuracy: 0.4255 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0840 - accuracy: 0.4138\n",
      "Epoch 00010: val_loss improved from 1.07609 to 1.07473, saving model to rong_test1_fer2.h5\n",
      "141/141 [==============================] - 51s 360ms/step - loss: 1.0840 - accuracy: 0.4138 - val_loss: 1.0747 - val_accuracy: 0.4235 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0762 - accuracy: 0.4255\n",
      "Epoch 00011: val_loss improved from 1.07473 to 1.07447, saving model to rong_test1_fer2.h5\n",
      "141/141 [==============================] - 48s 344ms/step - loss: 1.0762 - accuracy: 0.4255 - val_loss: 1.0745 - val_accuracy: 0.4279 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0783 - accuracy: 0.4227\n",
      "Epoch 00012: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 349ms/step - loss: 1.0783 - accuracy: 0.4227 - val_loss: 1.0778 - val_accuracy: 0.4206 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0765 - accuracy: 0.4291\n",
      "Epoch 00013: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 52s 372ms/step - loss: 1.0765 - accuracy: 0.4291 - val_loss: 1.0746 - val_accuracy: 0.4265 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0774 - accuracy: 0.4246\n",
      "Epoch 00014: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "141/141 [==============================] - 57s 403ms/step - loss: 1.0774 - accuracy: 0.4246 - val_loss: 1.0773 - val_accuracy: 0.4265 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0722 - accuracy: 0.4339\n",
      "Epoch 00015: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0722 - accuracy: 0.4339 - val_loss: 1.0758 - val_accuracy: 0.4309 - lr: 4.0000e-05\n",
      "Epoch 16/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0727 - accuracy: 0.4328\n",
      "Epoch 00016: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 354ms/step - loss: 1.0727 - accuracy: 0.4328 - val_loss: 1.0775 - val_accuracy: 0.4225 - lr: 4.0000e-05\n",
      "Epoch 17/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0745 - accuracy: 0.4362\n",
      "Epoch 00017: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0745 - accuracy: 0.4362 - val_loss: 1.0769 - val_accuracy: 0.4245 - lr: 4.0000e-05\n",
      "Epoch 18/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0777 - accuracy: 0.4282\n",
      "Epoch 00018: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 347ms/step - loss: 1.0777 - accuracy: 0.4282 - val_loss: 1.0759 - val_accuracy: 0.4270 - lr: 8.0000e-06\n",
      "Epoch 19/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0737 - accuracy: 0.4346\n",
      "Epoch 00019: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 351ms/step - loss: 1.0737 - accuracy: 0.4346 - val_loss: 1.0754 - val_accuracy: 0.4289 - lr: 8.0000e-06\n",
      "Epoch 20/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0784 - accuracy: 0.4233\n",
      "Epoch 00020: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "141/141 [==============================] - 51s 358ms/step - loss: 1.0784 - accuracy: 0.4233 - val_loss: 1.0768 - val_accuracy: 0.4270 - lr: 8.0000e-06\n",
      "Epoch 21/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0806 - accuracy: 0.4167\n",
      "Epoch 00021: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 51s 358ms/step - loss: 1.0806 - accuracy: 0.4167 - val_loss: 1.0769 - val_accuracy: 0.4260 - lr: 1.6000e-06\n",
      "Epoch 22/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0679 - accuracy: 0.4373\n",
      "Epoch 00022: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 351ms/step - loss: 1.0679 - accuracy: 0.4373 - val_loss: 1.0767 - val_accuracy: 0.4275 - lr: 1.6000e-06\n",
      "Epoch 23/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0738 - accuracy: 0.4346\n",
      "Epoch 00023: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "141/141 [==============================] - 51s 360ms/step - loss: 1.0738 - accuracy: 0.4346 - val_loss: 1.0758 - val_accuracy: 0.4279 - lr: 1.6000e-06\n",
      "Epoch 24/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0726 - accuracy: 0.4326\n",
      "Epoch 00024: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.0726 - accuracy: 0.4326 - val_loss: 1.0787 - val_accuracy: 0.4225 - lr: 3.2000e-07\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - ETA: 0s - loss: 1.0789 - accuracy: 0.4238\n",
      "Epoch 00025: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0789 - accuracy: 0.4238 - val_loss: 1.0754 - val_accuracy: 0.4279 - lr: 3.2000e-07\n",
      "Epoch 26/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0803 - accuracy: 0.4233\n",
      "Epoch 00026: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0803 - accuracy: 0.4233 - val_loss: 1.0763 - val_accuracy: 0.4265 - lr: 3.2000e-07\n",
      "Epoch 27/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0736 - accuracy: 0.4286\n",
      "Epoch 00027: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 354ms/step - loss: 1.0736 - accuracy: 0.4286 - val_loss: 1.0779 - val_accuracy: 0.4240 - lr: 6.4000e-08\n",
      "Epoch 28/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0745 - accuracy: 0.4300\n",
      "Epoch 00028: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0745 - accuracy: 0.4300 - val_loss: 1.0768 - val_accuracy: 0.4235 - lr: 6.4000e-08\n",
      "Epoch 29/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0768 - accuracy: 0.4222\n",
      "Epoch 00029: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "141/141 [==============================] - 50s 358ms/step - loss: 1.0768 - accuracy: 0.4222 - val_loss: 1.0764 - val_accuracy: 0.4279 - lr: 6.4000e-08\n",
      "Epoch 30/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0728 - accuracy: 0.4289\n",
      "Epoch 00030: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 51s 359ms/step - loss: 1.0728 - accuracy: 0.4289 - val_loss: 1.0776 - val_accuracy: 0.4245 - lr: 1.2800e-08\n",
      "Epoch 31/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0791 - accuracy: 0.4235\n",
      "Epoch 00031: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0791 - accuracy: 0.4235 - val_loss: 1.0756 - val_accuracy: 0.4289 - lr: 1.2800e-08\n",
      "Epoch 32/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0760 - accuracy: 0.4253\n",
      "Epoch 00032: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "141/141 [==============================] - 51s 359ms/step - loss: 1.0760 - accuracy: 0.4253 - val_loss: 1.0775 - val_accuracy: 0.4245 - lr: 1.2800e-08\n",
      "Epoch 33/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0754 - accuracy: 0.4277\n",
      "Epoch 00033: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 51s 359ms/step - loss: 1.0754 - accuracy: 0.4277 - val_loss: 1.0774 - val_accuracy: 0.4240 - lr: 2.5600e-09\n",
      "Epoch 34/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0754 - accuracy: 0.4342\n",
      "Epoch 00034: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 51s 361ms/step - loss: 1.0754 - accuracy: 0.4342 - val_loss: 1.0767 - val_accuracy: 0.4250 - lr: 2.5600e-09\n",
      "Epoch 35/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0789 - accuracy: 0.4202\n",
      "Epoch 00035: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0789 - accuracy: 0.4202 - val_loss: 1.0764 - val_accuracy: 0.4270 - lr: 2.5600e-09\n",
      "Epoch 36/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0739 - accuracy: 0.4286\n",
      "Epoch 00036: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0739 - accuracy: 0.4286 - val_loss: 1.0757 - val_accuracy: 0.4289 - lr: 5.1200e-10\n",
      "Epoch 37/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0766 - accuracy: 0.4224\n",
      "Epoch 00037: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 67s 472ms/step - loss: 1.0766 - accuracy: 0.4224 - val_loss: 1.0759 - val_accuracy: 0.4279 - lr: 5.1200e-10\n",
      "Epoch 38/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0729 - accuracy: 0.4302\n",
      "Epoch 00038: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
      "141/141 [==============================] - 60s 427ms/step - loss: 1.0729 - accuracy: 0.4302 - val_loss: 1.0767 - val_accuracy: 0.4260 - lr: 5.1200e-10\n",
      "Epoch 39/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0739 - accuracy: 0.4309\n",
      "Epoch 00039: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 68s 484ms/step - loss: 1.0739 - accuracy: 0.4309 - val_loss: 1.0764 - val_accuracy: 0.4270 - lr: 1.0240e-10\n",
      "Epoch 40/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0759 - accuracy: 0.4291\n",
      "Epoch 00040: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 68s 485ms/step - loss: 1.0759 - accuracy: 0.4291 - val_loss: 1.0768 - val_accuracy: 0.4260 - lr: 1.0240e-10\n",
      "Epoch 41/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0751 - accuracy: 0.4266\n",
      "Epoch 00041: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
      "141/141 [==============================] - 57s 404ms/step - loss: 1.0751 - accuracy: 0.4266 - val_loss: 1.0774 - val_accuracy: 0.4255 - lr: 1.0240e-10\n",
      "Epoch 42/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0736 - accuracy: 0.4339\n",
      "Epoch 00042: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 51s 359ms/step - loss: 1.0736 - accuracy: 0.4339 - val_loss: 1.0774 - val_accuracy: 0.4230 - lr: 2.0480e-11\n",
      "Epoch 43/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0778 - accuracy: 0.4264\n",
      "Epoch 00043: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 354ms/step - loss: 1.0778 - accuracy: 0.4264 - val_loss: 1.0769 - val_accuracy: 0.4255 - lr: 2.0480e-11\n",
      "Epoch 44/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0732 - accuracy: 0.4382\n",
      "Epoch 00044: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.0732 - accuracy: 0.4382 - val_loss: 1.0764 - val_accuracy: 0.4270 - lr: 2.0480e-11\n",
      "Epoch 45/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0700 - accuracy: 0.4393\n",
      "Epoch 00045: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0700 - accuracy: 0.4393 - val_loss: 1.0765 - val_accuracy: 0.4255 - lr: 4.0960e-12\n",
      "Epoch 46/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0760 - accuracy: 0.4251\n",
      "Epoch 00046: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0760 - accuracy: 0.4251 - val_loss: 1.0760 - val_accuracy: 0.4275 - lr: 4.0960e-12\n",
      "Epoch 47/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0765 - accuracy: 0.4293\n",
      "Epoch 00047: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 8.192000897078167e-13.\n",
      "141/141 [==============================] - 49s 347ms/step - loss: 1.0765 - accuracy: 0.4293 - val_loss: 1.0755 - val_accuracy: 0.4260 - lr: 4.0960e-12\n",
      "Epoch 48/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0727 - accuracy: 0.4357\n",
      "Epoch 00048: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0727 - accuracy: 0.4357 - val_loss: 1.0753 - val_accuracy: 0.4279 - lr: 8.1920e-13\n",
      "Epoch 49/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0710 - accuracy: 0.4459\n",
      "Epoch 00049: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0710 - accuracy: 0.4459 - val_loss: 1.0756 - val_accuracy: 0.4284 - lr: 8.1920e-13\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - ETA: 0s - loss: 1.0690 - accuracy: 0.4441\n",
      "Epoch 00050: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.6384001360475466e-13.\n",
      "141/141 [==============================] - 49s 349ms/step - loss: 1.0690 - accuracy: 0.4441 - val_loss: 1.0772 - val_accuracy: 0.4225 - lr: 8.1920e-13\n",
      "Epoch 51/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0758 - accuracy: 0.4280\n",
      "Epoch 00051: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0758 - accuracy: 0.4280 - val_loss: 1.0759 - val_accuracy: 0.4294 - lr: 1.6384e-13\n",
      "Epoch 52/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0736 - accuracy: 0.4342\n",
      "Epoch 00052: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0736 - accuracy: 0.4342 - val_loss: 1.0759 - val_accuracy: 0.4270 - lr: 1.6384e-13\n",
      "Epoch 53/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0729 - accuracy: 0.4313\n",
      "Epoch 00053: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.2768002178849846e-14.\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0729 - accuracy: 0.4313 - val_loss: 1.0771 - val_accuracy: 0.4250 - lr: 1.6384e-13\n",
      "Epoch 54/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0730 - accuracy: 0.4313\n",
      "Epoch 00054: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0730 - accuracy: 0.4313 - val_loss: 1.0747 - val_accuracy: 0.4294 - lr: 3.2768e-14\n",
      "Epoch 55/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0744 - accuracy: 0.4289\n",
      "Epoch 00055: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 354ms/step - loss: 1.0744 - accuracy: 0.4289 - val_loss: 1.0773 - val_accuracy: 0.4260 - lr: 3.2768e-14\n",
      "Epoch 56/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0740 - accuracy: 0.4291\n",
      "Epoch 00056: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 6.553600300244697e-15.\n",
      "141/141 [==============================] - 49s 351ms/step - loss: 1.0740 - accuracy: 0.4291 - val_loss: 1.0776 - val_accuracy: 0.4245 - lr: 3.2768e-14\n",
      "Epoch 57/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0768 - accuracy: 0.4331\n",
      "Epoch 00057: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 344ms/step - loss: 1.0768 - accuracy: 0.4331 - val_loss: 1.0762 - val_accuracy: 0.4284 - lr: 6.5536e-15\n",
      "Epoch 58/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0808 - accuracy: 0.4213\n",
      "Epoch 00058: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 344ms/step - loss: 1.0808 - accuracy: 0.4213 - val_loss: 1.0766 - val_accuracy: 0.4260 - lr: 6.5536e-15\n",
      "Epoch 59/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0748 - accuracy: 0.4317\n",
      "Epoch 00059: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.3107200431082805e-15.\n",
      "141/141 [==============================] - 49s 347ms/step - loss: 1.0748 - accuracy: 0.4317 - val_loss: 1.0771 - val_accuracy: 0.4240 - lr: 6.5536e-15\n",
      "Epoch 60/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0741 - accuracy: 0.4362\n",
      "Epoch 00060: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 346ms/step - loss: 1.0741 - accuracy: 0.4362 - val_loss: 1.0773 - val_accuracy: 0.4225 - lr: 1.3107e-15\n",
      "Epoch 61/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0704 - accuracy: 0.4353\n",
      "Epoch 00061: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0704 - accuracy: 0.4353 - val_loss: 1.0778 - val_accuracy: 0.4240 - lr: 1.3107e-15\n",
      "Epoch 62/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0773 - accuracy: 0.4260\n",
      "Epoch 00062: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 2.6214401285682084e-16.\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0773 - accuracy: 0.4260 - val_loss: 1.0756 - val_accuracy: 0.4275 - lr: 1.3107e-15\n",
      "Epoch 63/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0717 - accuracy: 0.4364\n",
      "Epoch 00063: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.0717 - accuracy: 0.4364 - val_loss: 1.0758 - val_accuracy: 0.4279 - lr: 2.6214e-16\n",
      "Epoch 64/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0746 - accuracy: 0.4282\n",
      "Epoch 00064: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 354ms/step - loss: 1.0746 - accuracy: 0.4282 - val_loss: 1.0769 - val_accuracy: 0.4245 - lr: 2.6214e-16\n",
      "Epoch 65/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0751 - accuracy: 0.4282\n",
      "Epoch 00065: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 5.2428803630155353e-17.\n",
      "141/141 [==============================] - 49s 349ms/step - loss: 1.0751 - accuracy: 0.4282 - val_loss: 1.0763 - val_accuracy: 0.4275 - lr: 2.6214e-16\n",
      "Epoch 66/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0760 - accuracy: 0.4291\n",
      "Epoch 00066: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0760 - accuracy: 0.4291 - val_loss: 1.0768 - val_accuracy: 0.4265 - lr: 5.2429e-17\n",
      "Epoch 67/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0752 - accuracy: 0.4331\n",
      "Epoch 00067: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 351ms/step - loss: 1.0752 - accuracy: 0.4331 - val_loss: 1.0777 - val_accuracy: 0.4230 - lr: 5.2429e-17\n",
      "Epoch 68/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0771 - accuracy: 0.4286\n",
      "Epoch 00068: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.0485760990728867e-17.\n",
      "141/141 [==============================] - 49s 349ms/step - loss: 1.0771 - accuracy: 0.4286 - val_loss: 1.0759 - val_accuracy: 0.4265 - lr: 5.2429e-17\n",
      "Epoch 69/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0717 - accuracy: 0.4402\n",
      "Epoch 00069: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 357ms/step - loss: 1.0717 - accuracy: 0.4402 - val_loss: 1.0756 - val_accuracy: 0.4284 - lr: 1.0486e-17\n",
      "Epoch 70/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0750 - accuracy: 0.4290\n",
      "Epoch 00070: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 347ms/step - loss: 1.0750 - accuracy: 0.4290 - val_loss: 1.0774 - val_accuracy: 0.4230 - lr: 1.0486e-17\n",
      "Epoch 71/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0789 - accuracy: 0.4198\n",
      "Epoch 00071: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 2.097152165058549e-18.\n",
      "141/141 [==============================] - 49s 349ms/step - loss: 1.0789 - accuracy: 0.4198 - val_loss: 1.0763 - val_accuracy: 0.4255 - lr: 1.0486e-17\n",
      "Epoch 72/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0748 - accuracy: 0.4306\n",
      "Epoch 00072: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.0748 - accuracy: 0.4306 - val_loss: 1.0782 - val_accuracy: 0.4221 - lr: 2.0972e-18\n",
      "Epoch 73/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0722 - accuracy: 0.4359\n",
      "Epoch 00073: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0722 - accuracy: 0.4359 - val_loss: 1.0764 - val_accuracy: 0.4270 - lr: 2.0972e-18\n",
      "Epoch 74/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.4280\n",
      "Epoch 00074: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 4.19430449555322e-19.\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0763 - accuracy: 0.4280 - val_loss: 1.0762 - val_accuracy: 0.4265 - lr: 2.0972e-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0773 - accuracy: 0.4193\n",
      "Epoch 00075: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0773 - accuracy: 0.4193 - val_loss: 1.0747 - val_accuracy: 0.4294 - lr: 4.1943e-19\n",
      "Epoch 76/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0729 - accuracy: 0.4315\n",
      "Epoch 00076: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 357ms/step - loss: 1.0729 - accuracy: 0.4315 - val_loss: 1.0766 - val_accuracy: 0.4265 - lr: 4.1943e-19\n",
      "Epoch 77/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0728 - accuracy: 0.4293\n",
      "Epoch 00077: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 8.388609197901593e-20.\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0728 - accuracy: 0.4293 - val_loss: 1.0765 - val_accuracy: 0.4260 - lr: 4.1943e-19\n",
      "Epoch 78/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0789 - accuracy: 0.4300\n",
      "Epoch 00078: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 354ms/step - loss: 1.0789 - accuracy: 0.4300 - val_loss: 1.0780 - val_accuracy: 0.4240 - lr: 8.3886e-20\n",
      "Epoch 79/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0699 - accuracy: 0.4375\n",
      "Epoch 00079: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 357ms/step - loss: 1.0699 - accuracy: 0.4375 - val_loss: 1.0766 - val_accuracy: 0.4245 - lr: 8.3886e-20\n",
      "Epoch 80/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0723 - accuracy: 0.4428\n",
      "Epoch 00080: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.6777218395803187e-20.\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.0723 - accuracy: 0.4428 - val_loss: 1.0757 - val_accuracy: 0.4275 - lr: 8.3886e-20\n",
      "Epoch 81/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0755 - accuracy: 0.4244\n",
      "Epoch 00081: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0755 - accuracy: 0.4244 - val_loss: 1.0777 - val_accuracy: 0.4235 - lr: 1.6777e-20\n",
      "Epoch 82/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0749 - accuracy: 0.4324\n",
      "Epoch 00082: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0749 - accuracy: 0.4324 - val_loss: 1.0764 - val_accuracy: 0.4250 - lr: 1.6777e-20\n",
      "Epoch 83/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0716 - accuracy: 0.4395\n",
      "Epoch 00083: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 3.3554436145371517e-21.\n",
      "141/141 [==============================] - 49s 349ms/step - loss: 1.0716 - accuracy: 0.4395 - val_loss: 1.0768 - val_accuracy: 0.4265 - lr: 1.6777e-20\n",
      "Epoch 84/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0746 - accuracy: 0.4244\n",
      "Epoch 00084: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 358ms/step - loss: 1.0746 - accuracy: 0.4244 - val_loss: 1.0771 - val_accuracy: 0.4250 - lr: 3.3554e-21\n",
      "Epoch 85/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0728 - accuracy: 0.4284\n",
      "Epoch 00085: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 358ms/step - loss: 1.0728 - accuracy: 0.4284 - val_loss: 1.0762 - val_accuracy: 0.4265 - lr: 3.3554e-21\n",
      "Epoch 86/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0780 - accuracy: 0.4209\n",
      "Epoch 00086: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 6.710887229074304e-22.\n",
      "141/141 [==============================] - 50s 358ms/step - loss: 1.0780 - accuracy: 0.4209 - val_loss: 1.0748 - val_accuracy: 0.4299 - lr: 3.3554e-21\n",
      "Epoch 87/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0737 - accuracy: 0.4277\n",
      "Epoch 00087: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0737 - accuracy: 0.4277 - val_loss: 1.0761 - val_accuracy: 0.4265 - lr: 6.7109e-22\n",
      "Epoch 88/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0737 - accuracy: 0.4313\n",
      "Epoch 00088: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 1.0737 - accuracy: 0.4313 - val_loss: 1.0759 - val_accuracy: 0.4284 - lr: 6.7109e-22\n",
      "Epoch 89/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0748 - accuracy: 0.4291\n",
      "Epoch 00089: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.3421774862045392e-22.\n",
      "141/141 [==============================] - 49s 349ms/step - loss: 1.0748 - accuracy: 0.4291 - val_loss: 1.0755 - val_accuracy: 0.4294 - lr: 6.7109e-22\n",
      "Epoch 90/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0797 - accuracy: 0.4169\n",
      "Epoch 00090: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 356ms/step - loss: 1.0797 - accuracy: 0.4169 - val_loss: 1.0763 - val_accuracy: 0.4265 - lr: 1.3422e-22\n",
      "Epoch 91/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0735 - accuracy: 0.4337\n",
      "Epoch 00091: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0735 - accuracy: 0.4337 - val_loss: 1.0780 - val_accuracy: 0.4245 - lr: 1.3422e-22\n",
      "Epoch 92/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0724 - accuracy: 0.4359\n",
      "Epoch 00092: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 2.684355073383274e-23.\n",
      "141/141 [==============================] - 50s 353ms/step - loss: 1.0724 - accuracy: 0.4359 - val_loss: 1.0761 - val_accuracy: 0.4250 - lr: 1.3422e-22\n",
      "Epoch 93/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0719 - accuracy: 0.4413\n",
      "Epoch 00093: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.0719 - accuracy: 0.4413 - val_loss: 1.0747 - val_accuracy: 0.4299 - lr: 2.6844e-23\n",
      "Epoch 94/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0729 - accuracy: 0.4351\n",
      "Epoch 00094: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0729 - accuracy: 0.4351 - val_loss: 1.0761 - val_accuracy: 0.4275 - lr: 2.6844e-23\n",
      "Epoch 95/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0753 - accuracy: 0.4279\n",
      "Epoch 00095: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 5.368710272984293e-24.\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 1.0753 - accuracy: 0.4279 - val_loss: 1.0777 - val_accuracy: 0.4230 - lr: 2.6844e-23\n",
      "Epoch 96/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0710 - accuracy: 0.4344\n",
      "Epoch 00096: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.0710 - accuracy: 0.4344 - val_loss: 1.0762 - val_accuracy: 0.4265 - lr: 5.3687e-24\n",
      "Epoch 97/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0742 - accuracy: 0.4344\n",
      "Epoch 00097: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 354ms/step - loss: 1.0742 - accuracy: 0.4344 - val_loss: 1.0760 - val_accuracy: 0.4255 - lr: 5.3687e-24\n",
      "Epoch 98/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0742 - accuracy: 0.4293\n",
      "Epoch 00098: val_loss did not improve from 1.07447\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 1.0737420861512948e-24.\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.0742 - accuracy: 0.4293 - val_loss: 1.0763 - val_accuracy: 0.4260 - lr: 5.3687e-24\n",
      "Epoch 99/100\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.0735 - accuracy: 0.4362\n",
      "Epoch 00099: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 49s 351ms/step - loss: 1.0735 - accuracy: 0.4362 - val_loss: 1.0761 - val_accuracy: 0.4270 - lr: 1.0737e-24\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - ETA: 0s - loss: 1.0786 - accuracy: 0.4262\n",
      "Epoch 00100: val_loss did not improve from 1.07447\n",
      "141/141 [==============================] - 50s 352ms/step - loss: 1.0786 - accuracy: 0.4262 - val_loss: 1.0766 - val_accuracy: 0.4265 - lr: 1.0737e-24\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples = 16944\n",
    "nb_validation_samples = 2099\n",
    "epochs=100\n",
    "\n",
    "history=model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(train_loss, label = 'Train Loss', color = 'gold')\n",
    "plt.plot(test_loss, label = 'Test Loss', color = 'orange')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_classifier = cv2.CascadeClassifier('../datasets/classifiers/haarcascade_frontalface_default.xml')\n",
    "# classifier =load_model('rong_test1_fer2.h5')\n",
    "\n",
    "# # class_labels = ['angry','disgust','happy','neutral','sad']\n",
    "# class_labels = ['angry','disgust','fear','happy','neutral','sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def face_detector(img):\n",
    "#     # Convert image to grayscale\n",
    "#     gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "#     if faces is ():\n",
    "#         return (0,0,0,0),np.zeros((48,48),np.uint8),img\n",
    "\n",
    "#     for (x,y,w,h) in faces:\n",
    "#         cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "#         roi_gray = gray[y:y+h,x:x+w]\n",
    "\n",
    "#     try:\n",
    "#         roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "#     except:\n",
    "#         return (x,w,y,h),np.zeros((48,48),np.uint8),img\n",
    "#     return (x,w,y,h),roi_gray,img\n",
    "\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "#     # Grab a single frame of video\n",
    "#     ret, frame = cap.read()\n",
    "#     labels = []\n",
    "#     gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "#     for (x,y,w,h) in faces:\n",
    "#         cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "#         roi_gray = gray[y:y+h,x:x+w]\n",
    "#         roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "#     # rect,face,image = face_detector(frame)\n",
    "\n",
    "\n",
    "#         if np.sum([roi_gray])!=0:\n",
    "#             roi = roi_gray.astype('float')/255.0\n",
    "#             roi = img_to_array(roi)\n",
    "#             roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "#         # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "#             preds = classifier.predict(roi)[0]\n",
    "#             label=class_labels[(preds.argmax())]\n",
    "#             label_position = (x,y)\n",
    "#             cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "#         else:\n",
    "#             cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "#     cv2.imshow('Emotion Detector',frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
